{"SUMMARY.html":{"url":"SUMMARY.html","title":"目 录","keywords":"","body":"目录 目 录 >> 自动化运维 首 页 Alerts 应用部署 Ansible 简介 示例 Docker Prometheus Nginx Consul PMM Redis in Docker Kubernetes 安装 Kubespray RKE Rancher 部署 K8s Dashboard ELK Prometheus 其它 MySQL主从复制 Percona toolkit yum安装MySQL yum安装RabbitMQ yum安装Redis 安装ruby和redis dump Y-API的安装步骤 GitLab升级 服务监控 Consul Consul Consul-template Nginx-Consul-template Consul-ACL Prometheus Prometheus 核心组件——prometheus 告警组件——alertmanager 图表展示——grafana 联邦 自动发现 常用软件接入Prometheus监控 MySQL MySQL主从同步 MongoDB Redis RabbitMQ NginX 自研应用接入Prometheus监控 Zabbix 自动发现 日志分析 ELK X-pack权限控制版ES ELK账号权限分组管理 ELK若干问题 清理ES索引 持续集成 CI/CD GitLab CI/CD GitLab Runner GitLab CI/CD 配置管理 CMDB Ansible Tower BlueKing 任务调度 简介 XXL-JOB XXL-JOB升级 >> 关于本书 制作 发布 官方示例 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"./":{"url":"./","title":"首 页","keywords":"","body":" 自动化运维 [x] 批量运维：减少重复劳动和人为失误 [x] 资源/服务监控：及时发现异常 [x] 持续集成：通过CI/CD工具，结合Docker、K8s，运用自动部署工具或API等手段，完成项目持续迭代，应用的持续部署 [x] 日志平台：性能分析、故障定位 [ ] 配置管理：通过代码使基础架构的配置和管理自动化 [ ] 代码审查：保证代码质量、代码安全 [ ] ... DevOps DevOps 是一种文化、一种理念。 or DevOps 的核心价值观 是 团队文化(Culture)、 自动化(Automation)、 评估(Measurement)和 分享(Sharing)（CAMS），同时，团队对 DevOps 的执行力也是 DevOps 能否成功的重要因素。 团队文化让大家团结一致 自动化是 DevOps 的基础 评估保证了及时的改进 分享让 CAMS 成为一个完整的循环过程 DevOps 的另一个思想是任何东西，包括服务器、数据库、网络、日志文件、应用配置、文档、自动化测试、部署流程等，都可以通过代码来管理。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/devops.html":{"url":"docs/devops.html","title":"Alerts","keywords":"","body":"YH - Alerts —— 告警看板&通知管理 URL：https://alerts.platform.production.keep.com Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/ansible/":{"url":"docs/deployment/ansible/","title":"Ansible","keywords":"","body":" Ansible is an IT automation tool. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates. Ansible 是一个IT自动化工具。它可以配置系统，部署软件以及编排更高级的IT任务，例如持续部署或零停机滚动更新。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/ansible/ansible-intro.html":{"url":"docs/deployment/ansible/ansible-intro.html","title":"简介","keywords":"","body":"Ansible简介 正如Ansible官网的标题“Ansible is Simple IT Automation”含义，Ansible简化了IT自动化的实现，是高效运维的一个重要工具。 Ansible的设计初衷就是在若干服务器上从零开始执行所有必需的配置与操作。例如，在启动Web 服务器之前先启动数据库，或者为了实现零停机升级，将Web 服务器逐一从负载均衡上摘除并升级。Ansible通过极简的模型来控制各种操作按照所需顺序执行。其它同类工具有：Chef、Puppet 及Saltstack。 架构 组件 功能 ansible core ansible自身核心模块 host inventory 主机清单，定义可管控的主机列表 connection plugins 连接插件，一般默认基于ssh协议连接 modules core modules（自带的核心模块）、custom modules（自定义模块） playbooks 剧本，按照所设定编排的顺序执行命令 特性 基于Python语言实现，由Paramiko（Python的一个可并发连接ssh主机功能库），PyYAML和Jinja2（模板化）三个关键模块实现 模块化设计，ansible自身是一个核心组件，调用特定的模块来完成特定任务 基于SSH协议工作，免客户端，有两种认证方式：密码、公钥 使用yaml语言编排剧本，连续任务按先后设置顺序完成 是一个声明式的管理工具，在编写脚本时使用的是声明式语言 声明式语言表示“期望目标是什么状态”，如果已经是则返回\"ok\"，否则改变目标状态并返回\"changed\"。 具有幂等性的特点，执行一次或多次，安装出来的环境是一样的 HTTP/1.1中对“幂等性”的定义是：一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）。 也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。 与SaltStack比较： 共同点： Ansible和SaltStack都是的目前最流行的自动化运维工具，能满足企业IT系统的自动化运维管理。这两个工具都是用python开发的，可以部署到不同的系统环境中和具有良好的二次开发特性。 在执行的命令的时候，Ansible和SaltStack都支持Ad-hoc操作模式，也可以支持将命令写入yaml格式文件中再批量执行。 在处理返回结果方面，Ansible和SaltStack的返回结果格式都是JSON格式，比较易懂和方便解析。 差异： 响应速度：SaltStack更快，在文件传输方面快一个量级，但在批量脚本执行、多机器部署方面相近 SaltStack的master和minion主机是通过ZeroMQ传输数据，而Ansible是通过标准SSH进行数据传输，SaltStack的响应速度要比Ansible快很多。标准SSH连接的时候比较耗费时间，ZeroMQ传输的速度会快很多，所以单单从响应速度方面考虑SaltStack会是更好的选择。但是在一般的运维场景下Ansible的响应速度也可以满足需求。 有测试数据表明，在执行命令、分发文件、读取文件方面，SaltStack的响应速度是Ansible的10倍左右。但在批量脚本执行、多机器部署方面，二者速度相当。 安全性：Ansible更安全 Ansible使用标准SSH连接传输数据，不需要在远程主机上启动守护进程，并且标准SSH数据传输本身就是加密传输，这样远程主机不容易被攻击。 自身运维：Ansible更友好 Ansible和远端主机之间的通过标准SSH通信，远程主机上只需要运行SSH进程就可以进行运维操作，而SSH是机房主机中一般都安装和启动的进程，所以Ansible对机房运维不会增加过多的运维成本。 基础知识 常用模块 Ansible的常用模块主要有以下十来个： 模块名称 功能 ping 尝试连接主机，如果测试成功会返回\"pong\" command 在远程节点执行命令 yum 使用yum软件包管理工具管理软件包 shell 和command模块类似，执行命令，支持变量等符号 cron 管理定时任务 service 管理程序服务 file 设置文件属性 copy 复制本地文件到远程主机 script 传送本地的一个脚本并在远程主机上执行 setup 获取远程主机的参数信息 user 管理用户账户 group 添加或者删除用户组 ... ... 配置文件 Ansible配置是以ini格式存储数据的，在Ansible中，几乎所有配置都可以通过Playbook或环境变量来重新赋值。加载配置文件的优先顺序如下： ANSIBLE_CONFIG：优先查找环境变量ANSIBLE_CONFIG指向的配置文件 ./ansible.cfg：当前目录下的ansible.cfg配置文件 ~/.ansible.cfg：当前用户home目录下的.ansible.cfg配置文件 /etc/ansible/ansible.cfg：安装ansible生成的默认配置文件 Ansible按顺序查找并应用最先找到的ansible配置。ansible.cfg中的配置可被playbook中的自定义配置覆盖 常用的配置参数： 配置项 含义 inventory 资源清单文件，就是一些Ansible需要连接管理的主机列表 library Ansible模块的安装目录 forks 默认并发进程数5，可根据控制主机的性能和被管理节点的数量调整 sudo_user 执行命令的用户，可在playbook中重新设置 remote_port SSH连接端口一般是22，如有个别特殊的，可在inventory中单独指定 host_key_checking 是否检查公钥，一般设置为false timeout SSH连接的超时间隔，默认20s log_path ansible日志路径 ... ... 执行过程 加载自己的配置文件，默认/etc/ansible/ansible.cfg 查找对应的主机配置文件，找到要执行的主机或者组 加载自己对应的模块文件，如 command 通过ansible将模块或命令生成对应的临时py文件(python脚本)， 并将该文件传输至远程服务器 对应执行用户的家目录的.ansible/tmp/XXX/XXX.PY文件 给文件 +x 执行权限 执行并返回结果 删除临时py文件，sleep 0退出 剧本（Playbook） playbook的构成： Target section：定义将要执行 playbook 的远程主机组 Variable section：定义 playbook 运行时需要使用的变量 Task section：定义将要在远程主机上执行的任务列表 Handler section：定义 task 执行完成以后需要调用的任务 一般所需的目录层： vars：变量层 tasks：任务层 handlers：触发条件 files：文件 template：模板 安装 以root用户安装ansible为例，方式如下： 从Github获取源码安装 需预先安装好Python2.5以上版本。另外，对于Ansible，Windows系统不可以做控制主机。如果控制主机没有预装Python，从Python官网选择2.7.15版本的源码包，解压到控制主机执行安装即可。 安装过Python环境之后，更新以下模块： # 安装python的包管理工具pip [root@VM_0_6_centos ~]# easy_install pip # 通过pip安装ansible必需的若干python模块 [root@VM_0_6_centos ~]# pip install paramiko PyYAML Jinja2 httplib2 six 下载、安装ansible git clone git://github.com/ansible/ansible.git --recursive cd ./ansible # Bash命令 source ./hacking/env-setup Yum安装 Fedora 用户可直接安装Ansible，但RHEL或CentOS用户，需要 配置 EPEL。 更新EPEL源 # rpm方式安装EPEL源 rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # 或者从阿里镜像源获取 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 然后更新一下缓存 [root@VM_0_6_centos ~]# yum clean all [root@VM_0_6_centos ~]# yum makecache 安装ansible [root@VM_0_6_centos ~]# yum install ansible -y 其它系统安装Ansible 其它操作系统，详见Ansible安装方法。 如何使用 我们主要使用ansible和ansible-playbook这两个命令。批量执行简单命令时，使用ansible；执行更复杂的批量安装部署时用ansible-playbook。以下是我在云主机上使用ansible的三个示例： 个人云主机主机环境如下： [root@VM_0_7_centos ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) [root@VM_0_7_centos ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 50G 6.8G 40G 15% / devtmpfs 911M 0 911M 0% /dev tmpfs 920M 24K 920M 1% /dev/shm tmpfs 920M 436K 920M 1% /run tmpfs 920M 0 920M 0% /sys/fs/cgroup tmpfs 184M 0 184M 0% /run/user/0 tmpfs 184M 0 184M 0% /run/user/1000 使用ansible 执行简单命令 安装好ansible之后，在当前目录编写一个hosts文件，定义一个主机群组vm，配置好待部署机器的主机别名、IP地址等信息，格式可以如下: cat myhosts [vm] vm01 ansible_host=192.168.xxx.xx1 # ansible2.0之前的版本应该使用ansible_ssh_host vm02 ansible_host=192.168.xxx.xx2 然后，通过指定-i参数指定自定义hosts文件执行一个简单的命令，如查询系统（redhat系列）版本：cat /etc/redhat-release 仅查询vm01 [root@VM_0_7_centos ~]# ansible -i myhosts vm01 -m shell -a \"cat /etc/redhat-release\" vm01 | CHANGED | rc=0 >> CentOS Linux release 7.3.1611 (Core) 查询主机群组vm中的所有主机 [root@VM_0_7_centos ~]# ansible -i myhosts vm -m shell -a \"cat /etc/redhat-release\" vm02 | CHANGED | rc=0 >> CentOS Linux release 7.3.1611 (Core) vm01 | CHANGED | rc=0 >> CentOS Linux release 7.3.1611 (Core) 查询hosts文件中的所有主机 [root@VM_0_7_centos ~]# ansible -i myhosts all -m shell -a \"cat /etc/redhat-release\" vm02 | CHANGED | rc=0 >> CentOS Linux release 7.3.1611 (Core) vm01 | CHANGED | rc=0 >> CentOS Linux release 7.3.1611 (Core) command模块不支持管道。例如，企图通过df -h |sed -n 2p获取第一个列出的文件系统的数据时，报错如下： [root@VM_0_7_centos ~]# ansible -i myhosts vm01 -m command -a \"df -h |sed -n 2p\" vm01 | FAILED | rc=1 >> df: invalid option -- 'n' Try 'df --help' for more information.non-zero return code 此时，可以使用shell模块替代，如下： [root@VM_0_7_centos ~]# ansible -i myhosts vm01 -m shell -a \"df -h | sed -n 2p\" vm01 | CHANGED | rc=0 >> /dev/vda1 50G 3.5G 44G 8% / 执行控制主机上的脚本 如果需要执行多条shell命令，而又不想编写playbook，可以将包含这些命令的shell脚本放在控制主机上，通过script模块执行。 cat /root/test.sh #!/bin/sh yum install httpd -y systemctl start httpd systemctl status httpd EOF # 在vm01上安装httpd并启动 [root@VM_0_7_centos ~]# ansible -i myhosts vm01 -m script -a \"/root/test.sh\" vm01 | CHANGED => { \"changed\": true, \"rc\": 0, ... 使用ansible-playbook 使用ansible-playbook部署一个tomcat集群 此例演示如何将一个Web应用部署到由 一个NginX节点 + 两个tomcat节点集群，然后模拟升级、回退过程： 1+2集群设定： 主机 应用 监听端口 版本 vm_nginx NginX 80 1.12 vm_tomcat_1 tomcat 8080 8.5.38 vm_tomcat_2 tomcat 8080 8.5.38 为了模拟升级、回退操作，准备两个Web应用： Web应用 版本 说明 examples.war 1.0 旧版本，回退用 examples.war 2.0 新版本，升级用 接下来，编写playbook。 在不是特别清楚每个步骤该如何编写的情况下，先一条一条以文字形式罗列出来： 更新所有节点的yum源 在NginX节点通过yum安装NginX 1.12 在tomcat节点通过yum安装Java 8 解压tomcat安装包到tomcat节点的安装目录 配置tomcat（添加环境变量、开机自启、开放8080端口等） 将examples.war v1.0发布到tomcat并启动tomcat（因初始已有examples，改为替换首页） 配置NginX并启动 停止vm_tomcat_1上的tomcat，将examples.war v2.0发布到tomcat并启动 等待vm_tomcat_1的tomcat启动正常 停止vm_tomcat_2上的tomcat，将examples.war v2.0发布到tomcat并启动 等待vm_tomcat_2的tomcat启动正常 重复步骤8~11，但改用examples.war v1.0，模拟回退过程 据此，可作以下规划。 角色设定： role 说明 包含步骤 tomcat 安装Java 8，tomcat 8，并替换examples首页 1，3 ~ 6 nginx 安装NginX 1.12 1，2，7 upgrade 升级到examples.war v2.0 8 ~ 11 rollback 回退到examples.war v1.0 12 主机组划分： group hosts roles lb vm_nginx nginx web vm_tomcat_1vm_tomcat_2 tomcatupgraderollback 入口playbook划分： name roles start.yml tomcatnginx upgrade.yml upgrade rollback.yml rollback 完整的playbook目录结构如下: [root@VM_0_7_centos test]# tree ansible_tomcat_cluster/ ansible_tomcat_cluster/ |-- ansible.cfg |-- group_vars | `-- web | `-- main.yaml # 对web组可见的变量（参数） |-- hosts `-- roles |-- nginx | |-- files | | `-- nginx.conf # 用于替换/etc/nginx/nginx.conf，已作好负载均衡配置 | `-- tasks | `-- main.yml # 每个tasks下的main.yml（或main.yaml)是对应role的主流程 |-- rollback | |-- files | | `-- examples.war # 此为1.0版本，真实环境中，一般会从远程版本库获取，此处是预先准备好 | |-- handlers | | `-- main.yml | `-- tasks | `-- main.yml |-- rollback.yml |-- start.yml # 执行此playbook，会对lb和web组分别执行nginx和tomcat role的主流程 |-- tomcat | |-- files | | `-- apache-tomcat-8.5.38.tar.gz # 预置的tomcat8解压安装包 | |-- tasks | | `-- main.yaml | |-- templates | | `-- index.html # 首页模板，用于替换examples的首页。此处演示模板的变量注入 | `-- vars | `-- main.yaml # 仅对role tomcat可见的变量（参数） |-- upgrade | |-- files | | `-- examples.war # 此为2.0版本 | |-- handlers | | `-- main.yml # 此处演示处理器的用法，由notify触发（见upgrade/tasks/main.yml） | `-- tasks | `-- main.yml `-- upgrade.yml inventory（即hosts文件）： [root@VM_0_7_centos test]# cat ansible_tomcat/hosts [web] vm_tomcat_1 ansible_host=118.24.217.91 mytitle=\"this is vm01\" vm_tomcat_2 ansible_host=188.131.133.107 mytitle=\"this is vm02\" [lb] vm_nginx ansible_host=188.131.133.107 role nginx的任务编排： --- - name: 安装NginX \"{{ nginx_version }}\" yum: name={{ item }} state=present with_items: - yum-utils - \"nginx-{{ nginx_version }}\" - name: 配置NginX copy: src=nginx.conf dest=/etc/nginx/ - name: 启动NginX service: name=nginx state=started enabled=yes - name: 启用防火墙 service: name=firewalld state=started enabled=yes - name: 开启80端口（临时） shell: firewall-cmd --zone=public --add-port=80/tcp - name: 等待80端口启动 wait_for: port: 80 state: started role tomcat的任务编排： [root@VM_0_7_centos test]# cat ansible_tomcat/roles/install/tasks/main.yaml --- - name: 安装Java8 yum: name={{ item }} state=present with_items: - yum-utils - java-1.8.0 - name: 将tomcat8解压缩到安装目录下 unarchive: src: \"{{ app_name }}.tar.gz\" dest: \"{{ install_path }}\" - name: 注册tomcat环境变量 lineinfile: dest: /etc/profile state: present line: \"{{ item }}\" with_items: - \"export CATALINA_HOME=/usr/local/apache-tomcat-8.5.38/\" - \"export PATH=$CATALINA_HOME/bin:$PATH\" - name: 检测tomcat安装目录是否存在 stat: path={{ tomcat_home }} # 此处演示register用法，可在后面的步骤中打印{{ check_tomcat_result }}以观察其值 register: check_tomcat_result failed_when: not check_tomcat_result.stat.exists - name: 替换examples首页 template: src=index.html dest=\"{{ tomcat_home }}/webapps/examples/index.html\" - name: 启动tomcat shell: nohup \"{{ tomcat_home }}/bin/startup.sh\" & - name: 启用防火墙 service: name=firewalld state=started enabled=yes - name: 开启8080端口（临时） shell: firewall-cmd --zone=public --add-port=8080/tcp - name: 等待8080端口启动 wait_for: port: 8080 state: started - name: 配置tomcat开机自启动 lineinfile: dest: /etc/profile state: present line: \"{{ item }}\" with_items: - \"{{ tomcat_home }}/bin/startup.sh &\" 执行nginx和tomcat的安装部署： [root@VM_0_7_centos test]# ansible-playbook roles/start.yml 结果如下图： 检验环境变量是否已写入/etc/profile： 检验tomcat开机自启动是否已写入/etc/rc.local： 检验index.html模板是否注入成功： 预置的index.html模板： 注入变量后，h3标签值变成了\"this is vm01\"（另一个节点是\"this is vm02\"）： 检验NginX负载均衡是否生效： 执行逐个节点升级： upgrade.yml： - hosts: web serial: 1 # serial值为1，表示一个一个节点地执行 roles: - { role: upgrade } upgrade/tasks/main.yml： - name: 升级 |关闭tomcat command: sh \"{{ tomcat_home }}/bin/shutdown.sh\" - name: 等待8080端口关闭 wait_for: port: 8080 state: stopped notify: clean tomcat cache # 通知handler \"clean tomcat cache\" 处理 （剩余略） handlers/main.yml： - name: clean tomcat cache file: path=\"{{ tomcat_home }}/work/Catalina\" state=absent ignore_errors: yes 模拟应用升级的过程过程如下： [root@VM_0_7_centos ansible_tomcat_cluster]# ansible-playbook roles/upgrade.yml 先对vm_tomat_1： 再对vm_tomcat_2： 升级后，从浏览器访问变成了2.0版本（两个节点的页面一致，未作区分）： 执行逐个节点回退： rollback剧本内容与upgrade相似，从略。 [root@VM_0_7_centos ansible_tomcat_cluster]# ansible-playbook roles/rollback.yml 回退过程类似升级过程，结果如下： 回退后，从浏览器访问变成了1.0版本： 一切符合预期。 至此，NginX 1 + Tomcat 2方式部署war包，然后模拟升级、回退的过程演示完毕。完整的playbook见gitlab仓库 P.s. 以上操作不是使用密码，需事先作SSH免密认证。 # 控制主机生成ssh密钥对，简单点，直接ssh-keygen不带任何参数 ssh-keygen # 发送给被控主机 ssh-copy-id root@localhost # 如果要在控制主机执行部署，需发送公钥给自己 ssh-copy-id root@ 其它模块的用法，及高级特性，请结合实际需求，阅读官方文档《Ansible User Guide》。 参考 Ansible User Guide Ansible中文权威指南 Ansible和SaltStack的比较和选型 （End） Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/ansible/ansible-examples.html":{"url":"docs/deployment/ansible/ansible-examples.html","title":"示例","keywords":"","body":"Playbooks ansible-playbooks仓库：http://git.pro.keep.com/devops/ansible-playbooks.git k8s 创建K8s集群，基于rancher，基于rancher rke，基于kubespray等方式。 tomcat-ansible 将一个Web应用部署到由 一个NginX节点 + 两个tomcat节点集群，然后模拟升级、回退过程的playbook示例。 consul 安装consul集群(未测试)。 # 修改myhosts，并配置好ansible免密登录 # 安装 ansible-playbook -i myhosts ins_consul.yaml redis 部署redis集群，RPM安装、编译安装redis主从结构或集群。 rabbitmq 部署rabbitmq ... Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/docker/":{"url":"docs/deployment/docker/","title":"Docker","keywords":"","body":" Docker 使用客户端 - 服务器架构。Docker客户端与Docker守护进程通信，后者负责构建，运行和分发Docker容器。Docker客户端和守护程序可以在同一系统上运行，也可以将Docker客户端连接到远程Docker守护程序。Docker客户端和守护程序使用REST API，通过UNIX套接字或网络接口进行通信。 Docker vs VM docker-ce for centos Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/docker/prometheus.html":{"url":"docs/deployment/docker/prometheus.html","title":"Prometheus","keywords":"","body":"Prometheus version: '3.0' services: alertmanager: image: prom/alertmanager volumes: - ./config/alertmanager:/etc/alertmanager ports: - \"9093:9093\" environment: SERVICE_NAME: alertmanager SERVICE_TAGS: \"alertmanager,http\" prometheus: image: prom/prometheus volumes: - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml - ./config/prometheus/rules:/etc/prometheus/rules - /etc/localtime:/etc/localtime environment: SERVICE_NAME: prometheus SERVICE_TAGS: \"prometheus-target,prometheus,http\" ports: - \"9090:9090\" grafana: image: grafana/grafana volumes: - ./config/grafana/grafana.ini:/etc/grafana/grafana.ini - /data/grafana:/var/lib/grafana environment: - \"GF_SERVER_ROOT_URL=http://home.grafana.com\" - \"GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}\" - \"SERVICE_NAME=grafana\" - \"SERVICE_TAGS=prometheus-target,grafana,http\" ports: - 3000:3000 node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro network_mode: host ports: - 9100:9100 environment: SERVICE_TAGS: \"prometheus-target\" command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - '--collector.filesystem.ignored-mount-points=\"^(/rootfs|/host|)/(sys|proc|dev|host|etc)($$|/)\"' - '--collector.filesystem.ignored-fs-types=\"^(sys|proc|auto|cgroup|devpts|ns|au|fuse\\.lxc|mqueue)(fs|)$$\"' Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/docker/nginx-consul.html":{"url":"docs/deployment/docker/nginx-consul.html","title":"Nginx Consul","keywords":"","body":"Nginx + Consul version: '3' services: consul: image: consul environment: SERVICE_8500_NAME: \"consul-ui\" SERVICE_8500_TAGS: \"consul,http,ui\" ports: - 8500:8500 network_mode: host volumes: - /data/consul/data:/consul/data - ./conf.d:/consul/config command: \"agent -server -bootstrap-expect 1 -ui -disable-host-node-id -client 0.0.0.0 -bind 172.17.0.13\" registrator: image: gliderlabs/registrator depends_on: - consul volumes: - /var/run/docker.sock:/tmp/docker.sock network_mode: host environment: CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN} command: -internal consul://127.0.0.1:8500 nginx-consul: image: nginx-consul:alpine build: . depends_on: - consul - registrator ports: - 80:80 network_mode: host volumes: - ./files/nginx.conf.ctmpl:/etc/nginx/nginx.conf.ctmpl environment: HOST_TYPE: ${HOST_TYPE} CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN} command: -consul-addr=127.0.0.1:8500 -wait=5s -template /etc/nginx/nginx.conf.ctmpl:/etc/nginx/conf.d/app.conf:/etc/nginx/nginx.sh Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/docker/pmm.html":{"url":"docs/deployment/docker/pmm.html","title":"PMM","keywords":"","body":"PMM Percona Monitoring and Management (PMM)是一款开源的MySQL、MongoDB性能监控工具，PMM客户端负责收集DB监控数据，PMM服务端从已连接的客户端拉取数据，并通过第三方软件Grafana展示图表。 version: '2' services: pmm-data: image: percona/pmm-server:1.1.1 container_name: pmm-data volumes: - /opt/prometheus/data - /opt/consul-data - /var/lib/mysql - /var/lib/grafana entrypoint: /bin/true pmm-server: image: percona/pmm-server:1.1.1 container_name: pmm-server ports: - 8088:80 restart: always environment: - SERVICE_80_NAME=pmm - SERVICE_80_TAGS=pmm,http - SERVER_USER=admin - SERVER_PASSWORD=${PMM_SERVER_PASSWORD} - METRICS_RETENTION=720h - METRICS_MEMORY=4194304 - METRICS_RESOLUTION=5s - QUERIES_RETENTION=30 volumes_from: - pmm-data 或者 #!/bin/sh tag0=$1 tag=${tag0:=1.1.1} docker stop pmm-server docker rm pmm-server pmm-data docker create \\ -v /opt/prometheus/data \\ -v /opt/consul-data \\ -v /var/lib/mysql \\ -v /var/lib/grafana \\ --name pmm-data \\ percona/pmm-server:$tag /bin/true docker run -d \\ -p 8088:80 \\ --volumes-from pmm-data \\ --name pmm-server \\ -e SERVER_USER=admin \\ -e SERVICE_80_NAME=pmm \\ -e SERVICE_80_TAGS=pmm,http,ui \\ -e SERVER_PASSWORD=${PMM_SERVER_PASSWORD} \\ --restart always \\ percona/pmm-server:$tag Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/docker/docker-redis.html":{"url":"docs/deployment/docker/docker-redis.html","title":"Redis in Docker","keywords":"","body":"redis [root@localhost redis]# docker inspect redis_redis_1 |grep net \"NetworkMode\": \"redis_localnet\", \"SandboxKey\": \"/var/run/docker/netns/a498b7651db0\", \"redis_localnet\": { [root@localhost redis]# docker run -it --network redis_localnet --rm redis redis-cli -h redis redis:6379> get k (nil) redis:6379> exit [root@localhost redis]# docker run -it --rm redis redis-cli -h 192.168.80.2 192.168.80.2:6379> set k abc OK 192.168.80.2:6379> get k \"abc\" 192.168.80.2:6379> exit [root@localhost redis]# docker run -it --network redis_localnet --rm redis redis-cli -h redis redis:6379> get k \"abc\" redis:6379> del k (integer) 1 redis:6379> 从其它节点访问： [root@localhost ~]# docker run -it --rm redis redis-cli -h 10.1.7.211 Unable to find image 'redis:latest' locally latest: Pulling from library/redis 27833a3ba0a5: Already exists cde8019a4b43: Pull complete 97a473b37fb2: Pull complete c6fe0dfbb7e3: Pull complete 39c8f5ba1240: Pull complete cfbdd870cf75: Pull complete Digest: sha256:000339fb57e0ddf2d48d72f3341e47a8ca3b1beae9bdcb25a96323095b72a79b Status: Downloaded newer image for redis:latest 10.1.7.211:6379> get k (nil) 10.1.7.211:6379> set k abcdefg OK 10.1.7.211:6379> exit 创建集群： #!/bin/sh para=\"\" port=8000 # 6个节点8001——8006 for i in `seq 1 6` do # host? 192.168.* # ipaddr=$(docker inspect new_redis_redis${i}_1 |jq '.[0].NetworkSettings.Networks.new_redis_default.IPAddress' |sed 's/\\\"//g') # bridge? 172.* ipaddr=$(docker inspect new_redis_redis${i}_1 |jq '.[0].NetworkSettings.Networks.bridge.IPAddress' |sed 's/\\\"//g') let port=port+1 echo $ipaddr:$port para=\"$para $ipaddr:$port\" done echo $para # 172.17.0.12:8001 172.17.0.8:8002 172.17.0.13:8003 172.17.0.9:8004 172.17.0.10:8005 172.17.0.11:8006 docker run --rm -it inem0o/redis-trib create --replicas 1 $para 查看集群： [root@localhost new_redis]# docker run --rm -it inem0o/redis-trib info 172.17.0.11:8006 172.17.0.8:8002@18002 (f015467b...) -> 178239 keys | 5462 slots | 1 slaves. 172.17.0.12:8001@18001 (17687354...) -> 178166 keys | 5461 slots | 1 slaves. 172.17.0.13:8003@18003 (2097e0e6...) -> 178106 keys | 5461 slots | 1 slaves. [OK] 534511 keys in 3 masters. 32.62 keys per slot on average. [root@localhost new_redis]# docker run --rm -it redis redis-cli -h 172.17.0.12 -p 8001 172.17.0.12:8001> cluster nodes 6104ad4f30a511d642aebb1bce8dd77bb5d10621 172.17.0.11:8006@18006 slave 2097e0e6a66c6adb6ddbe3bc783663d390a9360d 0 1557199119530 6 connected f9a309feda3b442dbf555712c7ff47026dc4700b 172.17.0.9:8004@18004 slave 1768735459c96a46722b1f0f038c125fa88aef83 0 1557199119000 4 connected f015467b0439b9694a6d633cc62a2502079e74f7 172.17.0.8:8002@18002 master - 0 1557199120532 2 connected 5461-10922 1768735459c96a46722b1f0f038c125fa88aef83 172.17.0.12:8001@18001 myself,master - 0 1557199118000 1 connected 0-5460 2097e0e6a66c6adb6ddbe3bc783663d390a9360d 172.17.0.13:8003@18003 master - 0 1557199120030 3 connected 10923-16383 8126fa4d990c4e08cdb7d3e2ebf59771445b3a1a 172.17.0.10:8005@18005 slave f015467b0439b9694a6d633cc62a2502079e74f7 0 1557199119530 5 connected redis-single docker-compose.yml: version: '3' services: redis: image: redis ports: - \"6379:6379\" volumes: - ./data:/data - ./conf/redis.conf:/etc/redis.conf networks: localnet: aliases: - my-redis-server command: [\"redis-server\", \"/etc/redis.conf\"] networks: localnet: redis-cluster 单节点多容器redis集群 docker-compose.yml: version: '3' services: redis1: image: redis-cluster restart: always volumes: - /data/redis/6381/data:/data environment: - REDIS_PORT=6381 ports: - '6381:6381' #服务端口 - '16381:16381' #集群端口 redis2: image: redis-cluster restart: always volumes: - /data/redis/6382/data:/data environment: - REDIS_PORT=6382 ports: - '6382:6382' - '16382:16382' redis3: image: redis-cluster restart: always volumes: - /data/redis/6383/data:/data environment: - REDIS_PORT=6383 ports: - '6383:6383' - '16383:16383' redis4: image: redis-cluster restart: always volumes: - /data/redis/6384/data:/data environment: - REDIS_PORT=6384 ports: - '6384:6384' - '16384:16384' redis5: image: redis-cluster restart: always volumes: - /data/redis/6385/data:/data environment: - REDIS_PORT=6385 ports: - '6385:6385' - '16385:16385' redis6: image: redis-cluster restart: always volumes: - /data/redis/6386/data:/data environment: - REDIS_PORT=6386 ports: - '6386:6386' - '16386:16386' redis.conf: #端口 port REDIS_PORT #开启集群 cluster-enabled yes #配置文件 cluster-config-file nodes.conf cluster-node-timeout 5000 #更新操作后进行日志记录 appendonly yes #设置主服务的连接密码 # masterauth #设置从服务的连接密码 # requirepass entrypoint.sh: #!/bin/sh #只作用于当前进程,不作用于其创建的子进程 set -e #$0--Shell本身的文件名 $1--第一个参数 $@--所有参数列表 # allow the container to be started with `--user` if [ \"$1\" = 'redis-server' -a \"$(id -u)\" = '0' ]; then sed -i 's/REDIS_PORT/'$REDIS_PORT'/g' /usr/local/etc/redis.conf chown -R redis . #改变当前文件所有者 exec gosu redis \"$0\" \"$@\" #gosu是sudo轻量级”替代品” fi exec \"$@\" Dockerfile: FROM redis RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo 'Asia/Shanghai' >/etc/timezone # ENV REDIS_PORT 8000 # ENV REDIS_PORT_NODE 18000 EXPOSE $REDIS_PORT # EXPOSE $REDIS_PORT_NODE COPY entrypoint.sh /usr/local/bin/ COPY redis.conf /usr/local/etc/ RUN chmod 777 /usr/local/etc/redis.conf RUN chmod +x /usr/local/bin/entrypoint.sh ENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"] CMD [\"redis-server\", \"/usr/local/etc/redis.conf\"] Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/":{"url":"docs/deployment/k8s/","title":"Kubernetes","keywords":"","body":" Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes (K8s) 是一个开源容器编排引擎，用于自动化容器化应用程序的部署，扩展和管理。 应用部署方式的演变历史：物理机部署 => 虚拟机部署 => 容器化部署 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/installation/":{"url":"docs/deployment/k8s/installation/","title":"安装","keywords":"","body":"记录了几种不同的K8s集群搭建方式： Kubespray Rancher RKE Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/installation/kubespray-k8s/kubespray-k8s.html":{"url":"docs/deployment/k8s/installation/kubespray-k8s/kubespray-k8s.html","title":"Kubespray","keywords":"","body":" kubespray项目地址: https://github.com/kubernetes-sigs/kubespray 官方参考：http://192.168.100.150/k8s/kubespray-k8s/0812/kubespray/#/docs/getting-started 项目分支：release-2.11 K8s版本：1.15.1 Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes (K8s) 是一个开源容器编排引擎，用于自动化容器化应用程序的部署，扩展和管理。 应用部署方式的演变历史：物理机部署 => 虚拟机部署 => 容器化部署 准备 因国外网络问题，安装前需作必要的修改，写成预处理脚本如下： pre-install.sh: #!/bin/sh ### 安装git并clone官方安装脚本 yum install git -y kube_version=v1.15.1 if [ ! -d kubespray ]; then git clone https://github.com/kubernetes-sigs/kubespray cd kubespray git checkout release-2.11 cd - fi cd kubespray if [ ! -d inventory/mycluster ]; then cp -r inventory/sample inventory/mycluster fi ### 更新pip及安装依赖 if [ \"$1\" = \"-U\" ]; then easy_install pip pip install pip==8.0.3 pip uninstall requests pip install -U pip pip install -U setuptools pip install -r requirements.txt fi ################### 所有修改仅涉及以下4个文件，可事先改好另存覆盖 #inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml #roles/download/defaults/main.yml #roles/kubespray-defaults/defaults/main.yaml #inventory/mycluster/ ################### 若不事先准备改好的文件，可使用以下命令修改： ### 改版本号 sed -i \"s/^kube_version:.*$/kube_version: $kube_version/\" roles/kubespray-defaults/defaults/main.yaml sed -i \"s/^kube_version:.*$/kube_version: $kube_version/\" roles/download/defaults/main.yml sed -i \"s/^kube_version:.*$/kube_version: $kube_version/\" inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml ### 改镜像源 kube_image_repo=mirrorgooglecontainers sed -i \"s/^kube_image_repo:.*$/kube_image_repo: $kube_image_repo/\" inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml sed -i \"s/^kube_image_repo:.*$/kube_image_repo: $kube_image_repo/\" roles/download/defaults/main.yml sed -i \"s/k8s.gcr.io/$kube_image_repo/g\" roles/download/defaults/main.yml sed -i \"s#gcr.io/google_containers#$kube_image_repo#g\" roles/download/defaults/main.yml sed -i \"s/gcr.io/$kube_image_repo/g\" roles/download/defaults/main.yml sed -i \"s#(k8s.)?gcr.io(/google_containers)?#$kube_image_repo#g\" roles/download/defaults/main.yml #nodelocaldns_version: 1.15.1 nodelocaldns_image_repo=lzzeng/k8s-dns-node-cache sed -i 's#^nodelocaldns_image_repo:.*$'\"#nodelocaldns_image_repo: $nodelocaldns_image_repo#\" roles/download/defaults/main.yml ### 改二进制文件下载地址 download_url=http://192.168.100.150/k8s/$kube_version sed -i 's#^kubeadm_download_url:.*$#kubeadm_download_url: '\\\"\"$download_url\"'/kubeadm\\\"#' roles/download/defaults/main.yml sed -i 's#^hyperkube_download_url:.*$#hyperkube_download_url: '\\\"\"$download_url\"'/hyperkube\\\"#' roles/download/defaults/main.yml sed -i 's#^calicoctl_download_url:.*$#calicoctl_download_url: '\\\"\"$download_url\"'/calicoctl-linux-amd64\\\"#' roles/download/defaults/main.yml #1.15.1 使用 3.7.3 sed -i 's#^etcd_download_url:.*$#etcd_download_url: '\\\"\"$download_url\"'/etcd-v3.3.10-linux-amd64.tar.gz\\\"#' roles/download/defaults/main.yml #1.15.1 使用 v3.3.10 sed -i 's#^cni_download_url:.*$#cni_download_url: '\\\"\"$download_url\"'/cni-plugins-linux-amd64-v0.8.1.tgz\\\"#' roles/download/defaults/main.yml #1.15.1 使用 v0.8.1 sed -i 's#^crictl_download_url:.*$#crictl_download_url: '\\\"\"$download_url\"'/critest-v1.15.0-linux-amd64.tar.gz\\\"#' roles/download/defaults/main.yml #1.15.1 使用 v1.15.0 ### 可以修改内存限制，使得1G内存也可以执行安装 #sed -i 's/^minimal_node_memory_mb:.*$/minimal_node_memory_mb: 500/' roles/kubernetes/preinstall/defaults/main.yml #sed -i 's/^minimal_master_memory_mb:.*$/minimal_master_memory_mb: 500/' roles/kubernetes/preinstall/defaults/main.yml #sed -i 's/ansible_memtotal_mb\\s*>=\\s*[0-9]\\+/ansible_memtotal_mb >= 500/' roles/kubernetes/preinstall/tasks/0020-verify-settings.yml ### docker加速 cat > inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml # Debian docker-ce repo docker_debian_repo_base_url: \"https://mirrors.aliyun.com/docker-ce/linux/debian\" docker_debian_repo_gpgkey: \"https://mirrors.aliyun.com/docker-ce/linux/debian/gpg\" dockerproject_apt_repo_base_url: \"https://mirrors.aliyun.com/docker-engine/apt/repo\" dockerproject_apt_repo_gpgkey: \"https://mirrors.aliyun.com/docker-engine/apt/gpg\" docker_options: \"--insecure-registry= --registry-mirror=https://registry.docker-cn.com --graph= \" EOF ### 打印ansible命令 echo \"再kubespray根目录执行以下命令安装或重置：\" echo ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml echo ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root reset.yml inventory/mycluster/hosts.ini示例： node1、node2是master调度节点，node1~node3是etcd节点，node3~node6是工作节点。 [all] node1 ansible_host=192.168.100.79 ip=192.168.100.79 node2 ansible_host=192.168.100.80 ip=192.168.100.80 node3 ansible_host=192.168.100.81 ip=192.168.100.81 node4 ansible_host=192.168.100.216 ip=192.168.100.216 node5 ansible_host=192.168.100.217 ip=192.168.100.217 node6 ansible_host=192.168.100.218 ip=192.168.100.218 [kube-master] node1 node2 [etcd] node1 node2 node3 [kube-node] node3 node4 node5 node6 [k8s-cluster:children] kube-master kube-node 安装 另找一个执行ansible的机器，并授权使其可以免密登录node1~node6： ssh-keygen # 这几条命令都会提示手动确认 ssh-copy-id root@node1 ssh-copy-id root@node2 ... 执行安装： # 执行上述准备好的预处理脚本（对应v1.15.1版本） sh pre-install.sh # 按上述修改好hosts.ini后，执行ansible编排任务，等待结束即可 ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml 登录 获取登录kubernetes-dashboard的token: kubectl get svc --all-namespaces | grep kubernetes-dashboard kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token 未使用keepalived时，可通过其一master登录：https://192.168.100.79:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ keepalived配置示例 node1 (master-1) 的/etc/keepalived/keepalived.conf： ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_haproxy { script \"killall -0 haproxy\" interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface ens32 virtual_router_id 78 priority 120 advert_int 1 authentication { auth_type PASS auth_pass xxxxxxxxxxxxxxxx } virtual_ipaddress { 192.168.100.78 } track_script { check_haproxy } } node2 (master-2) 的/etc/keepalived/keepalived.conf： ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_haproxy { script \"killall -0 haproxy\" interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens32 virtual_router_id 78 priority 100 advert_int 1 authentication { auth_type PASS auth_pass xxxxxxxxxxxxxxxx } virtual_ipaddress { 192.168.100.78 } track_script { check_haproxy } } 仅state BACKUP和priority 100两处不同。 补充（1.14.1） 修改了配置文件中的镜像地址 inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml roles/download/defaults/main.yml 大部分镜像可以改从registry.cn-hangzhou.aliyuncs.com/google_containers获取。 不能直接下载的软件 通过浏览器（已设置google代理）下载kubeadm和hyperkube，放置在内网文件服务器中。hyperkube和kubeadm包会被下载到所有k8s节点的/tmp/releases目录下。 # kubeadm_download_url: https://storage.googleapis.com/kubernetes-release/release/v1.14.1/bin/linux/amd64/kubeadm # hyperkube_download_url: https://storage.googleapis.com/kubernetes-release/release/v1.14.1/bin/linux/amd64/hyperkube 不能从Ali镜像库获取的镜像 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.4.0 k8s.gcr.io/k8s-dns-node-cache:1.15.1 执行 pip install -r requirements.txt ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml 如何清理 ansible -i inventory/mycluster/hosts.ini all -m script -a '/opt/clean.sh' # 或者 ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root reset.yml clean.sh: #!/bin/sh rm -rf /etc/kubernetes/ rm -rf /var/lib/kubelet rm -rf /var/lib/etcd rm -rf /usr/local/bin/kubectl rm -rf /etc/systemd/system/calico-node.service rm -rf /etc/systemd/system/kubelet.service systemctl stop etcd.service systemctl disable etcd.service systemctl stop calico-node.service systemctl disable calico-node.service docker stop $(docker ps -q) docker rm $(docker ps -a -q) systemctl restart docker 安装完成后修改节点标签 kubectl label node node3 node-role.kubernetes.io/worker=\"\" #标记worker kubectl label node node3 node-role.kubernetes.io/node=\"\" #标记node kubectl label node node3 \"node-role.kubernetes.io/worker\"- # 删除worker标记 Kubernetes Dashboard 以NodePort方式暴露服务 修改代码，使用NodePort方式访问Dashboard。 # ./roles/kubernetes-apps/ansible/templates/dashboard.yml.j2 # ------------------- Dashboard Service ------------------- # targetPort: 8443 type: NodePort //添加这一行 selector: k8s-app: kubernetes-dashboard 获取token kubectl get svc --all-namespaces | grep kubernetes-dashboard kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token 非NodePort方式访问 https://192.168.100.78:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/（192.168.100.78是VIP或者某个master节点IP） 后续操作 以上完成的是带有kubernetes dashboard的初始环境的搭建，之后还要进行 替换Ingress、设置AD认证、安装管理工具等，此不涉及。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/installation/rke-k8s/rke-k8s.html":{"url":"docs/deployment/k8s/installation/rke-k8s/rke-k8s.html","title":"RKE","keywords":"","body":"RKE安装K8s HA集群过程记录 准备工作 ansible主机清单： [rke] rke ansible_host=192.168.100.228 [k8s] master01 ansible_host=192.168.101.72 master02 ansible_host=192.168.101.75 master03 ansible_host=192.168.100.229 安装docker： ansible-playbook roles/docker.yml 使用ansible之前，需要分发密钥至各节点root用户。 创建rancher用户，并分发密钥： ansible-playbook roles/key.yml 安装rke、kubectl、helm工具： # https://www.cnrancher.com/download/rke/rke_linux-amd64 wget https://www.cnrancher.com/download/rke/rke_linux-amd64 chmod +x rke_linux-amd64 mv rke_linux-amd64 /usr/bin/rke # https://www.cnrancher.com/download/kubectl/kubectl_amd64-linux wget https://www.cnrancher.com/download/kubectl/kubectl_amd64-linux chmod +x kubectl_amd64-linux mv kubectl_amd64-linux /usr/bin/kubectl # https://www.cnrancher.com/download/helm/helm-linux.tar.gz wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.0-linux-amd64.tar.gz tar -xf helm-v2.12.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/helm mv linux-amd64/tiller /usr/bin/tiller rm -rf linux-amd64 创建集群 rancher-cluster.yml: nodes: - address: 192.168.101.72 user: rancher role: [controlplane,worker,etcd] - address: 192.168.101.75 user: rancher role: [controlplane,worker,etcd] - address: 192.168.100.229 user: rancher role: [controlplane,etcd] services: etcd: snapshot: true creation: 6h retention: 24h rke up: rke up --config rancher-cluster.yml rke up后会生成kube_config_rancher-cluster.yml 设置kube_config环境变量（或者复制到~/.kube/config ）： echo \"export KUBECONFIG=/home/rancher/kube_config_rancher-cluster.yml\" >> /etc/profile source /etc/profile 安装tiller # Helm在集群上安装tiller服务以管理charts. 由于RKE默认启用RBAC, 因此我们需要使用kubectl来创建一个serviceaccount，clusterrolebinding才能让tiller具有部署到集群的权限 kubectl -n kube-system create serviceaccount tiller # 创建ClusterRoleBinding以授予tiller帐户对集群的访问权限 kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller # 安装Helm Server(Tiller) helm init --service-account tiller --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 安装cert-manager helm install stable/cert-manager \\ --name cert-manager \\ --namespace kube-system 安装rancher web # 使用helm repo add命令添加Rancher chart仓库地址 helm repo add rancher-stable https://releases.rancher.com/server-charts/stable helm install rancher-stable/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=xxx.com helm install rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=xxx.com 如果不是通过DNS解析域名，而是通过本地hosts解析，可以通过给cattle-cluster-agent Pod和cattle-node-agent添加主机别名，让其可以正常通信，前提是IP地址可以互通。 kubectl -n cattle-system patch deployments cattle-cluster-agent --patch '{ \"spec\": { \"template\": { \"spec\": { \"hostAliases\": [ { \"hostnames\": [ \"xxx.com\" ], \"ip\": \"192.168.100.228\" } ] } } } }' # 上面这条命令可能报错：Error from server (NotFound): deployments.extensions \"cattle-cluster-agent\" not found，因为cattle-cluster-agent还没有创建成功 kubectl -n cattle-system patch daemonsets cattle-node-agent --patch '{ \"spec\": { \"template\": { \"spec\": { \"hostAliases\": [ { \"hostnames\": [ \"xxx.com\" ], \"ip\": \"192.168.100.228\" } ] } } } }' 安装rancher cli wget https://www.cnrancher.com/download/cli/rancher-linux-amd64.tar.gz mkdir rancher-linux-amd64.tmp.d # 临时目录 tar -xf rancher-linux-amd64.tar.gz -C rancher-linux-amd64.tmp.d find rancher-linux-amd64.tmp.d -name 'rancher' -type f | xargs -I {} mv {} /usr/bin/; rm -rf rancher-linux-amd64.tmp.d Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/installation/rancher-k8s/rancher-k8s.html":{"url":"docs/deployment/k8s/installation/rancher-k8s/rancher-k8s.html","title":"Rancher","keywords":"","body":"通过Rancher搭建K8s集群 集群组成 参考架构图： 主机列表： IP role description OS 192.168.101.71 rancher serverk8s-master (etcd, control) 控制节点rancher server centos7.616U24G下同 192.168.101.72 k8s-master (etcd, control) 控制节点 - 192.168.101.73 k8s-master (etcd, control) 控制节点 - 192.168.101.74 k8s-node (worker) worker节点（可被调度） centos7.616U48G下同 192.168.101.75 k8s-node (worker) worker节点 - 192.168.101.76 k8s-node (worker) worker节点 - 192.168.101.70 VIP 虚拟 IP - 安装步骤 环境设置 # /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.101.71 b71-k8s-m01 b71-k8s-m01.keep.com 192.168.101.72 b72-k8s-m02 b72-k8s-m02.keep.com 192.168.101.73 b73-k8s-m03 b73-k8s-m03.keep.com 192.168.101.74 b74-k8s-c01 b74-k8s-c01.keep.com 192.168.101.75 b75-k8s-c02 b75-k8s-c02.keep.com 192.168.101.76 b76-k8s-c03 b76-k8s-c03.keep.com # 关闭firewalld systemctl stop firewalld.service systemctl disable firewalld.service # 无swap分区，不涉及swapoff操作 # 关闭selinux setenforce 0 sed -i '/^SELINUX=/s/=.*$/=disabled/' /etc/selinux/config # 安装docker-ce 18.06 wget -P /etc/yum.repos.d https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install docker-ce-18.06.1.ce -y systemctl start docker.service systemctl enable docker.service haproxy + keepalived 配置： ### 对3个master节点：添加到文件/etc/sysctl.conf cat >/etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 EOF ### 修改keepalived.conf cat > /etc/keepalived/keepalived.conf mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:85:8a:00 brd ff:ff:ff:ff:ff:ff inet 192.168.101.72/24 brd 192.168.101.255 scope global noprefixroute ens192 valid_lft forever preferred_lft forever inet 192.168.101.70/32 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:fe85:8a00/64 scope link valid_lft forever preferred_lft forever ### 修改haproxy.cfg ... # main frontend which proxys to the backends #--------------------------------------------------------------------- frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver backend kubernetes-apiserver mode tcp balance roundrobin server b71-k8s-m01 192.168.101.71:6443 check weight 1 server b72-k8s-m02 192.168.101.72:6443 check weight 2 server b73-k8s-m03 192.168.101.73:6443 check weight 2 listen stats bind *:1080 stats auth admin:password stats refresh 5s stats realm HAProxy\\ Statistics stats uri /admin?stats # 启动haproxy systemctl enable haproxy.service systemctl start haproxy.service 需开放的端口见 https://rancher.com/docs/rancher/v2.x/en/installation/references/。 通过Rancher安装后，会自动配置iptables规则，查看rancher server节点的防火墙规则： [root@B71-k8s-m01 ~]# iptables -L -n Chain INPUT (policy ACCEPT) target prot opt source destination KUBE-EXTERNAL-SERVICES all -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW KUBE-FIREWALL all -- 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy DROP) target prot opt source destination KUBE-FORWARD all -- 0.0.0.0/0 0.0.0.0/0 DOCKER-USER all -- 0.0.0.0/0 0.0.0.0/0 ... 安装rancher server 在节点101.71上执行： docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:stable 登录Rancher Web创建集群 Url: https://192.168.101.71/ 添加master节点时，是按如下设置的： 复制命令在待添加的节点机器上执行，依次添加完3个master节点后如下，其中101.71是第二个添加的： 在机器数量足够的情况下，可将rancher server单独部署，此处是与K8s_master共用。经反复试验，添加master节点时，先添加101.72或101.73，不要首先添加rancher server所在节点101.71。否则，后续添加master时会出问题： 上图显示101.72一直处于Registering状态，不能添加进集群。 在添加节点异常时，先通过Web管理界面删除问题节点，然后在重新执行添加前，通过以下脚本清理一下待添加节点的环境： # 清理安装文件目录 DLIST=\"/var/lib/etcd /etc/kubernetes /etc/cni /opt/cni /var/lib/cni /var/run/calico /opt/rke\" for dir in $DLIST; do echo \"Removing $dir\" rm -rf $dir done # 清理docker CLIST=$(docker ps -qa) if [ \"x\"$CLIST == \"x\" ]; then echo \"No containers exist - skipping container cleanup\" else docker stop -f $CLIST && docker rm -f $CLIST fi ILIST=$(docker images -a -q) if [ \"x\"$ILIST == \"x\" ]; then echo \"No images exist - skipping image cleanup\" else docker rmi $ILIST fi VLIST=$(docker volume ls -q) if [ \"x\"$VLIST == \"x\" ]; then echo \"No volumes exist - skipping volume cleanup\" else docker volume rm -f $VLIST fi systemctl restart docker.service 下图是添加完3个master和3个node后的主机界面： 部署服务 测试部署5个jenkins实例： 访问 Jenkins： 执行kubectl命令行 通过Rancher Web界面来执行kubectl命令行： 如果直接登录主机，找不到kubectl命令，需进入相关的容器 添加Dashboard 找到kubernetes-dashboard，并部署（启动） Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/deployment/":{"url":"docs/deployment/k8s/deployment/","title":"部署","keywords":"","body":" Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes (K8s) 是一个开源容器编排引擎，用于自动化容器化应用程序的部署，扩展和管理。 应用部署方式的演变历史：物理机部署 => 虚拟机部署 => 容器化部署 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/deployment/Kubernetes-dashboard.html":{"url":"docs/deployment/k8s/deployment/Kubernetes-dashboard.html","title":"K8s Dashboard","keywords":"","body":"Kubernetes-dashboard 创建： kubectl apply -f 删除： kubectl -n kube-system delete $(kubectl -n kube-system get pod -o name | grep dashboard) 获取登录token： kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk '/^deployment-controller-token-/{print $1}') | awk '$1==\"token:\"{print $2}' kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep kubernetes-dashboard-token|awk '{print $1}')|grep token:|awk '{print $2}' kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token 创建ServiceAccount kubernetes-dashboard与cluster-admin role绑定： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/deployment/ELK.html":{"url":"docs/deployment/k8s/deployment/ELK.html","title":"ELK","keywords":"","body":"ELK elk-rc.yaml apiVersion: v1 kind: ReplicationController metadata: name: elk-rc labels: k8s-app: elk spec: selector: k8s-app: elk template: metadata: labels: k8s-app: elk spec: containers: - image: docker.elastic.co/elasticsearch/elasticsearch:6.6.0 name: elasticsearch ports: - containerPort: 9200 volumeMounts: - name: es-storage mountPath: /usr/share/elasticsearch/data subPath: elasticsearch/data - image: docker.elastic.co/logstash/logstash:6.6.0 name: logstash ports: - containerPort: 9600 - containerPort: 5000 - image: docker.elastic.co/kibana/kibana:6.6.0 name: kibana env: - name: ELASTICSEARCH_URL value: http://127.0.0.1:9200 ports: - containerPort: 5601 volumes: - name: es-storage persistentVolumeClaim: claimName: pvc001 elk-svc-kibana.yaml apiVersion: v1 kind: Service metadata: name: kibana labels: k8s-app: elk spec: type: LoadBalancer selector: k8s-app: elk ports: - port: 5601 name: kibana elk-svc-logstash.yaml apiVersion: v1 kind: Service metadata: name: logstash labels: k8s-app: elk spec: selector: k8s-app: elk ports: - port: 8600 name: logstash pv.yaml --- apiVersion: v1 kind: PersistentVolume metadata: name: pv001 spec: capacity: storage: 20Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: \"/path/to/nfs\" server: x.xx.xxx.xxx --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc001 spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: nfs 创建NFS持久卷： kubectl create -f pv.yaml 部署ELK： kubectl create -f elk-rc.yaml kubectl create -f elk-svc-logstash.yaml kubectl create -f elk-svc-kibana.yaml kubectl get pods kubectl get svc Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/k8s/deployment/k8s-prometheus.html":{"url":"docs/deployment/k8s/deployment/k8s-prometheus.html","title":"Prometheus","keywords":"","body":"K8s部署prometheus 示例配置 prometheus apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: kube-system data: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: ${1}://${2}${3} target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name --- apiVersion: apps/v1beta2 kind: Deployment metadata: labels: name: prometheus-deployment name: prometheus namespace: kube-system spec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.0.0 name: prometheus command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/prometheus\" - \"--storage.tsdb.retention=24h\" ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: \"/prometheus\" name: data - mountPath: \"/etc/prometheus\" name: config-volume resources: requests: cpu: 100m memory: 100Mi limits: cpu: 500m memory: 2500Mi serviceAccountName: prometheus volumes: - name: data emptyDir: {} - name: config-volume configMap: name: prometheus-config --- kind: Service apiVersion: v1 metadata: labels: app: prometheus name: prometheus namespace: kube-system spec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] - apiGroups: - extensions resources: - ingresses verbs: [\"get\", \"list\", \"watch\"] - nonResourceURLs: [\"/metrics\"] verbs: [\"get\"] --- apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: kube-system grafana apiVersion: extensions/v1beta1 kind: Deployment metadata: name: grafana-core namespace: kube-system labels: app: grafana component: core spec: replicas: 1 template: metadata: labels: app: grafana component: core spec: containers: - image: grafana/grafana:4.2.0 name: grafana-core imagePullPolicy: IfNotPresent # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: \"true\" - name: GF_AUTH_ANONYMOUS_ENABLED value: \"false\" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: \"true\" readinessProbe: httpGet: path: /login port: 3000 # initialDelaySeconds: 30 # timeoutSeconds: 1 volumeMounts: - name: grafana-persistent-storage mountPath: /var volumes: - name: grafana-persistent-storage emptyDir: {} --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: grafana namespace: kube-system spec: rules: - host: k8s.grafana http: paths: - path: / backend: serviceName: grafana servicePort: 3000 --- apiVersion: v1 kind: Service metadata: name: grafana namespace: kube-system labels: app: grafana component: core spec: type: NodePort ports: - port: 3000 selector: app: grafana component: core Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/":{"url":"docs/deployment/else/","title":"其它","keywords":"","body":"常用软件安装、配置 CentOS yum安装mysql-5.7 安装redis-5 安装ruby和redis dump ... Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/mysql-replication.html":{"url":"docs/deployment/else/mysql-replication.html","title":"MySQL主从复制","keywords":"","body":"MySQL主从配置 mysql配置 master端my.cnf配置示例： [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake ## 设置server_id，一般设置为IP，同一局域网内注意要唯一 server_id=100 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，可以随便取，最好有含义（关键就是这里了） log-bin=edu-mysql-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 slave端my.cnf配置示例： [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake ## 设置server_id，一般设置为IP,注意要唯一 server_id=101 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log-bin=edu-mysql-slave1-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=edu-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## 防止改变数据(除了特殊的线程) read_only=1 master端添加slave端复制账号 CREATE USER 'slave'@'%' IDENTIFIED BY 'xxxxxx'; #slave密码xxxxxx，下面change master时用到 GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; # mysql 5.7也可以一行命令 # GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%' IDENTIFIED BY 'xxxxxx'; # 'slave'@'%' 或 'slave'@'172.17.0.3' #172.17.0.3是slave容器的ip # 但不能写虚拟机的ip（本实验是通过docker-compose非host网络模式创建2个mysql实例，未验证host模式是否可以） 获取master信息 # mysql -uroot -p ... flush tables with read lock; # 锁表，start slave后解锁 UNLOCK TABLES show master status; +----------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +----------------------+----------+--------------+------------------+-------------------+ | edu-mysql-bin.000001 | 769 | | mysql slave端设定master change master to master_host='172.17.0.2', master_user='slave', master_password='xxxxxx', master_port=3306, master_log_file='edu-mysql-bin.000001', master_log_pos=769, master_connect_retry=30; # 172.17.0.3是master的IP，docker中通过docker inspect查看 # change master to master_host='172.17.0.2'时172.17.0.2是master容器的IP，不能使用虚拟机的IP ### change master to master_host='172.17.0.2', master_user='slave', master_password='slave', master_port=3306, master_log_file='edu-mysql-bin.000033', master_log_pos=72831102, master_connect_retry=30; slave启动、查看 mysql> start slave; mysql> show slave status \\G; *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.17.0.3 Master_User: slave Master_Port: 3306 Connect_Retry: 30 Master_Log_File: edu-mysql-bin.000001 Read_Master_Log_Pos: 29661 Relay_Log_File: edu-mysql-relay-bin.000018 Relay_Log_Pos: 6051 Relay_Master_Log_File: edu-mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/percona-toolkit.html":{"url":"docs/deployment/else/percona-toolkit.html","title":"Percona toolkit","keywords":"","body":"Percona toolkit 安装 wget https://www.percona.com/downloads/percona-toolkit/2.2.7/RPM/percona-toolkit-2.2.7-1.noarch.rpm yum install percona-toolkit-2.2.7-1.noarch.rpm -y yum install perl-Digest-MD5 -y Installed: percona-toolkit.noarch 0:2.2.7-1 Dependency Installed: perl.x86_64 4:5.16.3-294.el7_6 perl-Carp.noarch 0:1.26-244.el7 perl-Compress-Raw-Bzip2.x86_64 0:2.061-3.el7 perl-Compress-Raw-Zlib.x86_64 1:2.061-4.el7 perl-DBD-MySQL.x86_64 0:4.023-6.el7 perl-DBI.x86_64 0:1.627-4.el7 perl-Data-Dumper.x86_64 0:2.145-3.el7 perl-Encode.x86_64 0:2.51-7.el7 perl-Exporter.noarch 0:5.68-3.el7 perl-File-Path.noarch 0:2.09-2.el7 perl-File-Temp.noarch 0:0.23.01-3.el7 perl-Filter.x86_64 0:1.49-3.el7 perl-Getopt-Long.noarch 0:2.40-3.el7 perl-HTTP-Tiny.noarch 0:0.033-3.el7 perl-IO-Compress.noarch 0:2.061-2.el7 perl-IO-Socket-IP.noarch 0:0.21-5.el7 perl-IO-Socket-SSL.noarch 0:1.94-7.el7 perl-Mozilla-CA.noarch 0:20130114-5.el7 perl-Net-Daemon.noarch 0:0.48-5.el7 perl-Net-LibIDN.x86_64 0:0.12-15.el7 perl-Net-SSLeay.x86_64 0:1.55-6.el7 perl-PathTools.x86_64 0:3.40-5.el7 perl-PlRPC.noarch 0:0.2020-14.el7 perl-Pod-Escapes.noarch 1:1.04-294.el7_6 perl-Pod-Perldoc.noarch 0:3.20-4.el7 perl-Pod-Simple.noarch 1:3.28-4.el7 perl-Pod-Usage.noarch 0:1.63-3.el7 perl-Scalar-List-Utils.x86_64 0:1.27-248.el7 perl-Socket.x86_64 0:2.010-4.el7 perl-Storable.x86_64 0:2.45-3.el7 perl-Text-ParseWords.noarch 0:3.29-4.el7 perl-Time-HiRes.x86_64 4:1.9725-3.el7 perl-Time-Local.noarch 0:1.2300-2.el7 perl-constant.noarch 0:1.27-2.el7 perl-libs.x86_64 4:5.16.3-294.el7_6 perl-macros.x86_64 4:5.16.3-294.el7_6 perl-parent.noarch 1:0.225-244.el7 perl-podlators.noarch 0:2.5.1-3.el7 perl-threads.x86_64 0:1.87-4.el7 perl-threads-shared.x86_64 0:1.43-6.el7 Complete! 权限 GRANT ALL PRIVILEGES ON `zabbix`.* TO 'pt'@'%' show grants for 'root'@'192.168.1.101'; revoke SELECT, INSERT, UPDATE, DELETE, CREATE, PROCESS, SUPER, REPLICATION SLAVE ON *.* FROM 'root'@'192.168.1.101'; 执行 pt-table-checksum #pt-table-checksum --nocheck-replication-filters --no-check-binlog-format --replicate=test_sync.checksums --create-replicate-table --databases=test_sync --tables=contact h=10.1.7.211,u=root,p=mysqlxxx,P=3306 pt-table-checksum --nocheck-replication-filters --no-check-binlog-format --replicate=test_sync.checksums --create-replicate-table --databases=test_sync h=10.1.7.211,u=root,p=mysqlxxx,P=3306 pt-table-sync #pt-table-sync --replicate=test_sync.checksums h=10.1.7.211,u=root,p=mysqlxxx,P=3306 h=10.1.7.211,u=root,p=mysqlxxx,P=3307 --print pt-table-sync --replicate=test_sync.checksums h=10.1.7.211,u=root,p=mysqlxxx,P=3306 h=10.1.7.211,u=root,p=mysqlxxx,P=3307 --execute err: Can't make changes on the master because no unique index exists at /usr/bin/pt-table-sync line 10649. while doing zabbix.history on 172.17.0.3 pt-heartbeat #pt-heartbeat --user=root --ask-pass --host=10.1.7.211 --port 3306 --create-table -D test_sync --interval=1 --update --replace --daemonize #pt-heartbeat -D test_sync --table=heartbeat --monitor --host=10.1.7.211 --port=3306 --user=root --password=mysqlxxx --master-server-id=100 --print-master-server-id pt-heartbeat -D test_sync --table=heartbeat --check --host=10.1.7.211 --port=3306 --user=root --password=mysqlxxx --master-server-id=100 --print-master-server-id #pt-heartbeat --stop Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/ins_mysql.html":{"url":"docs/deployment/else/ins_mysql.html","title":"yum安装MySQL","keywords":"","body":"Yum安装MySQL-5.7 系统：CentOS 7.6 # 先从官网下载rpm包 wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-server-5.7.26-1.el7.x86_64.rpm # 安装yum依赖源 # rpm -Uvh https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm rpm -Uvh mysql80-community-release-el7-3.noarch.rpm # 修改yum源 sed -i 's/enabled=1/enabled=0/g' /etc/yum.repos.d/mysql-community.repo sed -i '/\\[mysql57-community\\]/,+3s/enabled=.*/enabled=1/' /etc/yum.repos.d/mysql-community.repo # 当前rpm包所在路径，安装 yum install mysql-community-server-5.7.26-1.el7.x86_64.rpm -y 完整日志： + wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-server-5.7.26-1.el7.x86_64.rpm --2019-08-13 16:48:01-- https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-server-5.7.26-1.el7.x86_64.rpm Resolving dev.mysql.com (dev.mysql.com)... 137.254.60.11 Connecting to dev.mysql.com (dev.mysql.com)|137.254.60.11|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-server-5.7.26-1.el7.x86_64.rpm [following] --2019-08-13 16:48:06-- https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-community-server-5.7.26-1.el7.x86_64.rpm Resolving cdn.mysql.com (cdn.mysql.com)... 23.44.160.128 Connecting to cdn.mysql.com (cdn.mysql.com)|23.44.160.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 173541272 (166M) [application/x-redhat-package-manager] Saving to: ‘mysql-community-server-5.7.26-1.el7.x86_64.rpm’ 100%[===========================================================================================>] 173,541,272 1.71MB/s in 1m 44s 2019-08-13 16:49:51 (1.59 MB/s) - ‘mysql-community-server-5.7.26-1.el7.x86_64.rpm’ saved [173541272/173541272] + wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm --2019-08-13 16:49:51-- https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm Resolving dev.mysql.com (dev.mysql.com)... 137.254.60.11 Connecting to dev.mysql.com (dev.mysql.com)|137.254.60.11|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm [following] --2019-08-13 16:49:53-- https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm Resolving repo.mysql.com (repo.mysql.com)... 104.89.31.15 Connecting to repo.mysql.com (repo.mysql.com)|104.89.31.15|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 26024 (25K) [application/x-redhat-package-manager] Saving to: ‘mysql80-community-release-el7-3.noarch.rpm’ 100%[===========================================================================================>] 26,024 49.0KB/s in 0.5s 2019-08-13 16:49:55 (49.0 KB/s) - ‘mysql80-community-release-el7-3.noarch.rpm’ saved [26024/26024] + rpm -Uvh mysql80-community-release-el7-3.noarch.rpm warning: mysql80-community-release-el7-3.noarch.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY Preparing... ################################# [100%] Updating / installing... 1:mysql80-community-release-el7-3 ################################# [100%] + sed -i s/enabled=1/enabled=0/g /etc/yum.repos.d/mysql-community.repo + sed -i '/\\[mysql57-community\\]/,+3s/enabled=.*/enabled=1/' /etc/yum.repos.d/mysql-community.repo + yum install mysql-community-server-5.7.26-1.el7.x86_64.rpm -y Loaded plugins: fastestmirror, langpacks Examining mysql-community-server-5.7.26-1.el7.x86_64.rpm: mysql-community-server-5.7.26-1.el7.x86_64 Marking mysql-community-server-5.7.26-1.el7.x86_64.rpm to be installed Resolving Dependencies --> Running transaction check ---> Package mysql-community-server.x86_64 0:5.7.26-1.el7 will be installed --> Processing Dependency: mysql-community-common(x86-64) = 5.7.26-1.el7 for package: mysql-community-server-5.7.26-1.el7.x86_64 Loading mirror speeds from cached hostfile mysql57-community | 2.5 kB 00:00:00 mysql57-community/x86_64/primary_db | 184 kB 00:00:02 --> Processing Dependency: mysql-community-client(x86-64) >= 5.7.9 for package: mysql-community-server-5.7.26-1.el7.x86_64 --> Processing Dependency: libnuma.so.1(libnuma_1.1)(64bit) for package: mysql-community-server-5.7.26-1.el7.x86_64 --> Processing Dependency: libnuma.so.1(libnuma_1.2)(64bit) for package: mysql-community-server-5.7.26-1.el7.x86_64 --> Processing Dependency: libnuma.so.1()(64bit) for package: mysql-community-server-5.7.26-1.el7.x86_64 --> Running transaction check ---> Package mysql-community-client.x86_64 0:5.7.27-1.el7 will be installed --> Processing Dependency: mysql-community-libs(x86-64) >= 5.7.9 for package: mysql-community-client-5.7.27-1.el7.x86_64 ---> Package mysql-community-common.x86_64 0:5.7.26-1.el7 will be installed ---> Package numactl-libs.x86_64 0:2.0.9-7.el7 will be installed --> Running transaction check ---> Package mariadb-libs.x86_64 1:5.5.60-1.el7_5 will be obsoleted --> Processing Dependency: libmysqlclient.so.18()(64bit) for package: 2:postfix-2.10.1-7.el7.x86_64 --> Processing Dependency: libmysqlclient.so.18(libmysqlclient_18)(64bit) for package: 2:postfix-2.10.1-7.el7.x86_64 ---> Package mysql-community-libs.x86_64 0:5.7.27-1.el7 will be obsoleting --> Running transaction check ---> Package mysql-community-libs-compat.x86_64 0:5.7.27-1.el7 will be obsoleting --> Finished Dependency Resolution Dependencies Resolved ===================================================================================================================================== Package Arch Version Repository Size ===================================================================================================================================== Installing: mysql-community-libs x86_64 5.7.27-1.el7 mysql57-community 2.2 M replacing mariadb-libs.x86_64 1:5.5.60-1.el7_5 mysql-community-libs-compat x86_64 5.7.27-1.el7 mysql57-community 2.0 M replacing mariadb-libs.x86_64 1:5.5.60-1.el7_5 mysql-community-server x86_64 5.7.26-1.el7 /mysql-community-server-5.7.26-1.el7.x86_64 746 M Installing for dependencies: mysql-community-client x86_64 5.7.27-1.el7 mysql57-community 24 M mysql-community-common x86_64 5.7.26-1.el7 mysql57-community 274 k numactl-libs x86_64 2.0.9-7.el7 os 29 k Transaction Summary ===================================================================================================================================== Install 3 Packages (+3 Dependent packages) Total size: 774 M Total download size: 29 M Downloading packages: warning: /var/cache/yum/x86_64/7/mysql57-community/packages/mysql-community-common-5.7.26-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY Public key for mysql-community-common-5.7.26-1.el7.x86_64.rpm is not installed (1/5): mysql-community-common-5.7.26-1.el7.x86_64.rpm | 274 kB 00:00:03 (2/5): mysql-community-client-5.7.27-1.el7.x86_64.rpm | 24 MB 00:00:04 (3/5): numactl-libs-2.0.9-7.el7.x86_64.rpm | 29 kB 00:00:00 (4/5): mysql-community-libs-compat-5.7.27-1.el7.x86_64.rpm | 2.0 MB 00:00:00 (5/5): mysql-community-libs-5.7.27-1.el7.x86_64.rpm | 2.2 MB 00:00:04 ------------------------------------------------------------------------------------------------------------------------------------- Total 3.6 MB/s | 29 MB 00:00:08 Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql Importing GPG key 0x5072E1F5: Userid : \"MySQL Release Engineering \" Fingerprint: a4a9 4068 76fc bd3c 4567 70c8 8c71 8d3b 5072 e1f5 Package : mysql80-community-release-el7-3.noarch (installed) From : /etc/pki/rpm-gpg/RPM-GPG-KEY-mysql Running transaction check Running transaction test Transaction test succeeded Running transaction Warning: RPMDB altered outside of yum. Installing : mysql-community-common-5.7.26-1.el7.x86_64 1/7 Installing : mysql-community-libs-5.7.27-1.el7.x86_64 2/7 Installing : mysql-community-client-5.7.27-1.el7.x86_64 3/7 Installing : numactl-libs-2.0.9-7.el7.x86_64 4/7 Installing : mysql-community-server-5.7.26-1.el7.x86_64 5/7 Installing : mysql-community-libs-compat-5.7.27-1.el7.x86_64 6/7 Erasing : 1:mariadb-libs-5.5.60-1.el7_5.x86_64 7/7 Verifying : mysql-community-server-5.7.26-1.el7.x86_64 1/7 Verifying : numactl-libs-2.0.9-7.el7.x86_64 2/7 Verifying : mysql-community-libs-compat-5.7.27-1.el7.x86_64 3/7 Verifying : mysql-community-client-5.7.27-1.el7.x86_64 4/7 Verifying : mysql-community-common-5.7.26-1.el7.x86_64 5/7 Verifying : mysql-community-libs-5.7.27-1.el7.x86_64 6/7 Verifying : 1:mariadb-libs-5.5.60-1.el7_5.x86_64 Installed: mysql-community-libs.x86_64 0:5.7.27-1.el7 mysql-community-libs-compat.x86_64 0:5.7.27-1.el7 mysql-community-server.x86_64 0:5.7.26-1.el7 Dependency Installed: mysql-community-client.x86_64 0:5.7.27-1.el7 mysql-community-common.x86_64 0:5.7.26-1.el7 numactl-libs.x86_64 0:2.0.9-7.el7 Replaced: mariadb-libs.x86_64 1:5.5.60-1.el7_5 Complete! Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/ins_rabbitmq.html":{"url":"docs/deployment/else/ins_rabbitmq.html","title":"yum安装RabbitMQ","keywords":"","body":"Yum安装RabbitMQ 系统：CentOS 7.6 wget https://github.com/rabbitmq/erlang-rpm/releases/download/v22.0.1/erlang-22.0.1-1.el7.x86_64.rpm wget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.7.15/rabbitmq-server-3.7.15-1.el7.noarch.rpm yum install erlang-22.0.1-1.el7.x86_64.rpm rabbitmq-server-3.7.15-1.el7.noarch.rpm Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/ins_redis5.html":{"url":"docs/deployment/else/ins_redis5.html","title":"yum安装Redis","keywords":"","body":"安装redis-5 系统：CentOS 7.6 rpm -Uvh http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh https://centos7.iuscommunity.org/ius-release.rpm yum install redis5-5.0.5 -y Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/ins_ruby_redis_dump.html":{"url":"docs/deployment/else/ins_ruby_redis_dump.html","title":"安装ruby和redis dump","keywords":"","body":"安装ruby-2.6和redis-dump 系统：CentOS 7.6 # 1. 安装ruby-install wget https://github.com/postmodern/ruby-install/archive/v0.7.0.tar.gz tar -xf v0.7.0.tar.gz cd ruby-install-0.7.0/ make install # 安装ruby ruby-install ruby # 安装redis-dump gem install redis-dump # ln -s /opt/rubies/ruby-2.6.3/bin/* /usr/bin/ export PATH=/opt/rubies/ruby-2.6.3/bin:$PATH Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/ins_yapi.html":{"url":"docs/deployment/else/ins_yapi.html","title":"Y-API的安装步骤","keywords":"","body":"Y-API 的安装步骤 安装 NodeJS # 安装 nodeJs wget http://192.168.100.150/else/node-v10.15.3-linux-x64.tar.xz tar -xf node-v10.15.3-linux-x64.tar.xz -C /usr/local/ chown -R root:root /usr/local/node-v10.15.3-linux-x64 # 设置环境变量 /etc/profile export PATH=/usr/local/node-v10.15.3-linux-x64/bin:$PATH # 检查node版本 node --version 安装 MongoDB # 安装 mongodb cat /etc/yum.repos.d/mongo.repo [mongodb-org-4.0] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc EOF yum install -y mongodb-org 配置 /etc/mongod.conf： systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log storage: dbPath: /var/lib/mongo journal: enabled: true processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo net: port: 27017 bindIp: 127.0.0.1 # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting. 启动 mongod systemctl enable mongod # 开机自启动 systemctl start mongod 安装 yapi-cli # 安装 yapi-cli npm --registry https://registry.npm.taobao.org install express # 前台测试启动yapi cd /usr/local/node-v10.15.3-linux-x64/my-yapi/ node vendors/server/app.js 配置 supervisor # 如果无easy_install命令 #wget http://192.168.100.150/else/setuptools-41.0.1.zip #unzip setuptools-41.0.1.zip #cd setuptools-41.0.1/ #python setup.py install #cd .. #rm -rf setuptools-41.0.1 # 如果无pip命令 #easy_install pip #pip install -U pip # pip安装 pip install supervisor echo_supervisord_conf > /etc/supervisord.conf 配置 /etc/supervisord.d/yapi.ini： [program:yapi] command=/usr/local/node-v10.15.3-linux-x64/bin/node vendors/server/app.js directory=/usr/local/node-v10.15.3-linux-x64/my-yapi user=root startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/yapi.log supervisor启动yapi systemctl enable supervisord.service # supervisord 开机自启 supervisord -c /etc/supervisord.conf # 重启yapi supervisorctl restart yapi 配置 NginX upstream yapi443 { #ip_hash; server 127.0.0.1:3000; } server { listen 443 ssl; server_name yapi.keep.com; root html; index index.html index.htm; ssl_certificate /etc/letsencrypt/live/keep.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/keep.com/privkey.pem; access_log /var/log/nginx/yapi.keep.com.443.access.log main; error_log /var/log/nginx/yapi.keep.com.443.error.log ; location / { proxy_pass http://yapi443; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect http://$host http://$host:$server_port; add_header Strict-Transport-Security \"max-age=31536000\"; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/deployment/else/gitlab-update.html":{"url":"docs/deployment/else/gitlab-update.html","title":"GitLab升级","keywords":"","body":"GitLab版本升级 先备份gitlab gitlab-rake gitlab:backup:create 生成的备份文件在：/var/opt/gitlab/backups/下，格式如：1537856454_gitlab_backup.tar 如果要恢复时间戳为1537856454的备份 gitlab-rake gitlab:backup:restore BACKUP=1537856454 升级到11.9.1（当前11.0.4） wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-11.9.1-ce.0.el7.x86_64.rpm rpm -Uvh gitlab-ce-11.9.1-ce.0.el7.x86_64.rpm Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/consul/":{"url":"docs/monitoring/consul/","title":"Consul","keywords":"","body":" Consul 是一个分布式服务发现与配置的工具。与其他分布式服务注册与发现的方案，Consul的方案更“一站式”，内置了服务注册与发现框架、分布一致性协议实现（不需要ZooKeeper）、健康检查、K/V存储、多数据中心方案。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/consul/consul.html":{"url":"docs/monitoring/consul/consul.html","title":"Consul","keywords":"","body":"Consul 当前consul集群（192.168.100.140-142）用作服务发现和健康检查，配合Prometheus、NginX完成动态配置。 API 以Consul API为准。我们主要用了以下两个： 注册服务（Register Service） curl http://192.168.100.140:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -d '{ \"Name\": \"test-name\", \"Tags\": [ \"test-tag\" ], \"EnableTagOverride\": false, \"ID\": \"test-id\", \"Meta\": {\"version\": \"1.0\"}, \"Address\": \"192.168.100.150\", \"Port\": 8080, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"Args\": [], \"HTTP\": \"http://192.168.100.150:8080/\", \"Interval\": \"15s\" } }' 注销服务（Deregister Service） curl -X PUT http://192.168.100.140:8500/v1/agent/service/deregister/test-id Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/consul/consul-template.html":{"url":"docs/monitoring/consul/consul-template.html","title":"Consul-template","keywords":"","body":"Consul-Template 配置consul client 在当前NginX节点安装Consul，并作为client加入consul集群。 安装consul-template wget https://releases.hashicorp.com/consul-template/0.20.0/consul-template_0.20.0_linux_amd64.tgz tar -xf consul-template_0.20.0_linux_amd64.tgz mv consul-template /usr/bin chmod a+x /usr/bin/consul-template 注册服务 curl http://192.168.100.140:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -d '{ \"ID\": \"test.zlz.01\", \"Name\": \"test_zlz\", \"Tags\": [\"zlz\"], \"Address\": \"192.168.101.35\", \"Port\": 9003, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"Args\": [], \"HTTP\": \"http://192.168.101.35:9003/hc\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"EnableTagOverride\": false }' curl http://192.168.100.140:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -d '{ \"ID\": \"test.zlz.02\", \"Name\": \"test_zlz\", \"Tags\": [\"zlz\"], \"Address\": \"192.168.101.36\", \"Port\": 9003, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"Args\": [], \"HTTP\": \"http://192.168.101.36:9003/hc\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"EnableTagOverride\": false }' 编写NginX配置模板 nginx.conf.ctmpl： {{range services -}} {{$name := .Name}} {{$service := service .Name}} {{if in .Tags \"zlz\"}} # 此例仅选择tag中包含zlz的, 下同; 各服务根据实际定义的tag自行修改 upstream {{$name}} { {{range $service}}server {{.Address}}:{{.Port}}; {{end}} } {{end}} {{end}} {{- range services -}} {{$name := .Name}} {{if in .Tags \"zlz\"}} server { listen 80; # server_name自行修改 server_name xxx.keep.com; root html; index index.html index.htm; access_log /var/log/nginx/{{$name}}_access.log main; location / { proxy_pass http://{{$name}}; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } {{end}} {{end}} 编写consul-template配置文件 nginx.hcl： consul { # 当前nginx节点的内网IP, 同时也是consul client节点 address = \"192.168.100.150:8500\" } template { # nginx配置模板 source = \"nginx.conf.ctmpl\" # nginx配置输出路径 destination = \"/etc/nginx/conf.d/default.conf\" command = \"service nginx reload\" } 测试consul-template consul-template -config nginx.hcl # 会生成/etc/nginx/conf.d/default.conf # [control] + c 或者不使用.hcl配置文件： nohup consul-template -template \"nginx.conf.ctmpl:/etc/nginx/conf.d/default.conf:nginx -s reload\" > consul-template.out 2>&1 & 测试自动更新NginX配置 运行consul-template服务 nohup consul-template -config nginx.hcl >/var/log/consul-template.log 2>&1 & 模拟健康状态异常时自动更新NginX配置 比如，修改\"HTTP\": \"http://192.168.101.35:9003/hc\"为\"HTTP\": \"http://192.168.101.35:9003/hcc\"，重新注册该服务后test.zlz.01的服务健康状态变为不正常。consul-template能自动检测到异常，并重新生成/etc/nginx/conf.d/default.conf。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/consul/my-nginx-consul.html":{"url":"docs/monitoring/consul/my-nginx-consul.html","title":"Nginx-Consul-template","keywords":"","body":"构建nginx-consul-template Tree: |-- docker-compose.yml |-- Dockerfile |-- files | |-- consul-template_0.20.0_linux_amd64.zip | |-- nginx.conf | |-- nginx.conf.ctmpl | `-- nginx.sh Dockerfile: FROM nginx:alpine # RUN apk update && \\ RUN apk add --no-cache unzip ENV CONSUL_TEMPLATE_VERSION 0.20.0 ENV PACKAGE consul-template_0.20.0_linux_amd64.zip # ADD https://releases.hashicorp.com/consul-template/${CONSUL_TEMPLATE_VERSION}/consul-template_${CONSUL_TEMPLATE_VERSION}_linux_amd64.zip /tmp/consul-template.zip ADD files/nginx.conf files/nginx.conf.ctmpl files/nginx.sh files/${PACKAGE} /etc/nginx/ RUN unzip /etc/nginx/${PACKAGE} -d /usr/bin && \\ chmod +x /usr/bin/consul-template && \\ rm -f /etc/nginx/${PACKAGE} && \\ chmod +x /etc/nginx/nginx.sh && \\ apk del unzip WORKDIR /etc/nginx ENTRYPOINT [\"/usr/bin/consul-template\"] nginx.sh: #!/bin/sh if nginx -t >/dev/null; then if [ -s /var/run/nginx.pid ]; then nginx -s reload if [ $? != 0 ]; then rm -f /var/run/nginx.pid nginx -c /etc/nginx/nginx.conf fi else nginx -c /etc/nginx/nginx.conf fi fi 修改nginx.conf: ... http { server_names_hash_bucket_size 128; ... nginx.conf.ctmpl: 含http标签的会被选择 {{range services}} {{$name := .Name}} {{$service := service .Name}} {{if in .Tags \"http\"}} upstream {{$name}} { {{range $service}}server {{.Address}}:{{.Port}}; {{end}} } {{end}} {{end}} {{range services}} {{$name := .Name}} {{if in .Tags \"http\"}} server { listen 80; server_name {{$name}}.{{env \"HOST_TYPE\"}}.keep.com; server_name {{$name}}.keep.com; root html; index index.html index.htm; access_log /var/log/nginx/{{$name}}_access.log main; location / { proxy_pass http://{{$name}}; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } {{end}} {{end}} docker-compose.yml: version: '3' services: consul: image: consul container_name: consul environment: SERVICE_8500_NAME: consul SERVICE_8500_TAGS: \"consul,http\" ports: - 8400:8400 - 8500:8500 - 8600:53/udp command: agent -server -client=0.0.0.0 -dev -node=node0 -bootstrap-expect=1 -data-dir=/tmp/consul registrator: image: gliderlabs/registrator depends_on: - consul volumes: - /var/run/docker.sock:/tmp/docker.sock command: -internal consul://consul:8500 nginx-consul: image: my-nginx-consul:alpine build: . depends_on: - consul - registrator ports: - 80:80 volumes: - ./files/nginx.conf.ctmpl:/etc/nginx/nginx.conf.ctmpl environment: HOST_TYPE: ${HOST_TYPE} command: -consul-addr=consul:8500 -wait=3s -template /etc/nginx/nginx.conf.ctmpl:/etc/nginx/conf.d/app.conf:/etc/nginx/nginx.sh Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/consul/consul-acl.html":{"url":"docs/monitoring/consul/consul-acl.html","title":"Consul-ACL","keywords":"","body":"Consul ACL systemd service [Unit] Description=consul agent Requires=network-online.target After=network-online.target [Service] EnvironmentFile=-/etc/sysconfig/consul Environment=GOMAXPROCS=2 Restart=on-failure ExecStart=/usr/local/bin/consul agent -config-dir=/opt/consul/conf -rejoin ExecReload=/bin/kill -HUP $MAINPID KillSignal=SIGTERM [Install] WantedBy=multi-user.target acl.json 启用ACL，需在配置加载目录下添加acl配置文件 { \"acl_datacenter\": \"dc1\", \"acl_master_token\": \"xxxxxxxxx\", \"acl_default_policy\": \"deny\", \"acl_down_policy\": \"extend-cache\" } deregitster curl -X PUT http://192.168.100.140:8500/v1/agent/service/deregister/ -H \"X-Consul-Token:xxx\" register 仅node check curl http://10.1.100.57:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -H \"X-Consul-Token:xxx\" -d '{ \"ID\": \"test.swms.api3\", \"Name\": \"test-swms-api3\", \"Tags\": [\"test\", \"swms\"], \"Address\": \"192.168.101.35\", \"Port\": 9003, \"Meta\": { \"version\": \"1.0\" }, \"EnableTagOverride\": false }' node check + service check curl http://10.1.100.57:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -H \"X-Consul-Token:xxxxxxxxxxx\" -d '{ \"ID\": \"test.swms.api2\", \"Name\": \"test-swms-api2\", \"Tags\": [\"test\", \"swms\"], \"Address\": \"192.168.101.35\", \"Port\": 9003, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"Args\": [], \"HTTP\": \"http://192.168.101.35:9003/hc\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"EnableTagOverride\": false }' Name不能包含小数点 创建一个agent token: curl \\ --request PUT \\ --header \"X-Consul-Token: xxxxxxxxxxxxxxxx\" \\ --data \\ '{ \"Name\": \"Agent Token\", \"Type\": \"client\", \"Rules\": \"node \\\"\\\" { policy = \\\"write\\\" } service \\\"\\\" { policy = \\\"read\\\" }\" }' http://127.0.0.1:8500/v1/acl/create Type: client WebUI token的policy可以设置为只读: service_prefix \"\" { policy = \"read\" } key_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } 示例： [root@VM_0_4_centos ins_exporter]# cat deReg.sh curl -i --request PUT \\ --header \"X-Consul-Token: xxxxxxxxxxxxxxxxxx\" \\ http://10.1.100.57:8500/v1/agent/service/deregister/$1 [root@VM_0_4_centos ins_exporter]# sh deReg.sh id-xxl-vm04 HTTP/1.1 200 OK Vary: Accept-Encoding Date: Sun, 07 Jul 2019 08:45:42 GMT Content-Length: 0 [root@VM_0_4_centos ins_exporter]# cat regXXL.sh curl http://10.1.100.57:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" \\ --header \"X-Consul-Token: xxxxxxxxxxxxxxxxxx\" -d '{ \"ID\": \"id-xxl-vm04\", \"Name\": \"xxl\", \"Tags\": [\"xxl-job\", \"http\"], \"Address\": \"192.168.100.150\", \"Port\": 9977, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"HTTP\": \"http://baidu.com\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\", \"type\": \"app\", \"hostname\": \"vm03\" }, \"EnableTagOverride\": false }' [root@VM_0_4_centos ins_exporter]# sh regXXL.sh HTTP/1.1 200 OK Vary: Accept-Encoding Date: Sun, 07 Jul 2019 08:47:47 GMT Content-Length: 0 # 不带token被禁止 [root@VM_0_4_centos ins_exporter]# curl -i -X PUT http://10.1.100.57:8500/v1/agent/service/deregister/id-xxl-vm04 HTTP/1.1 403 Forbidden Vary: Accept-Encoding Date: Sun, 07 Jul 2019 08:48:54 GMT Content-Length: 17 Content-Type: text/plain; charset=utf-8 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/":{"url":"docs/monitoring/prometheus/","title":"Prometheus","keywords":"","body":"Prometheus 在Docker中运行Prometheus监控组件，包括prometheus, alertmanager, grafana, 及相关exporter，实现基本的业务监控。 下图是官方网站上的Prometheus完整架构图： Prometheus配置（开发环境） 配置文件目录： [root@docker-dev docker-svc]# tree -C -L 4 prometheus/ prometheus/ ├── config │ ├── alertmanager │ │ ├── alertmanager.yml │ │ └── template │ │ └── test.tmpl │ └── prometheus │ ├── prometheus.yml │ └── rules │ └── default.yml └── docker-compose.yml docker-compose.yml: version: '3.0' services: alertmanager: image: prom/alertmanager volumes: - ./config/alertmanager:/etc/alertmanager ports: - \"9093:9093\" environment: SERVICE_NAME: alertmanager SERVICE_TAGS: \"alertmanager,http\" prometheus: image: prom/prometheus volumes: - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml - ./config/prometheus/rules:/etc/prometheus/rules - /etc/localtime:/etc/localtime environment: SERVICE_NAME: prometheus SERVICE_TAGS: \"prometheus-target,prometheus,http\" ports: - 9090:9090 grafana: image: grafana/grafana volumes: # - grafana-storage:/var/lib/grafana - /data/grafana:/var/lib/grafana environment: - \"GF_SERVER_ROOT_URL=http://grafana.server.name\" - \"GF_SECURITY_ADMIN_PASSWORD=xxxxxx\" - \"SERVICE_NAME=grafana\" - \"SERVICE_TAGS=prometheus-target,grafana,http\" ports: - 3000:3000 node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro network_mode: host ports: - 9100:9100 environment: SERVICE_TAGS: \"prometheus-target\" command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - '--collector.filesystem.ignored-mount-points=\"^(/rootfs|/host|)/(sys|proc|dev|host|etc)($$|/)\"' - '--collector.filesystem.ignored-fs-types=\"^(sys|proc|auto|cgroup|devpts|ns|au|fuse\\.lxc|mqueue)(fs|)$$\"' Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/":{"url":"docs/monitoring/prometheus/prom/","title":"Prometheus","keywords":"","body":" 主要涉及的Prometheus组件：prometheus的prometheus、alertmanager，相关exporter，及grafana提供的图表工具 核心组件 Prometheus 告警组件 Alertmanager 图表组件 Grafana 联邦 自动发现 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/prometheus-intro.html":{"url":"docs/monitoring/prometheus/prom/prometheus-intro.html","title":"核心组件——prometheus","keywords":"","body":"Prometheus监控 环境：CentOS Linux release 7.5.1804 (Core)、5.5.60-MariaDB 运行方式：Prometheus，Grafana和Alertmanager通过docker安装运行，MySQL和exporter直接运行在VM上 配置方式：静态配置 安装（静态配置） docker pull prom/prometheus docker pull grafana/grafana docker pull prom/alertmanager [root@VM_0_2_centos config]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE prom/prometheus latest f57ed0abd85c 2 days ago 109MB grafana/grafana latest ffd9c905f698 8 days ago 241MB prom/alertmanager latest 02e0d8e930da 6 weeks ago 42.5MB 执行prometheus前，先配置好以下文件。 [root@zabbix02 ~]# tree config/ config/ ├── alertmanager │ ├── alertmanager.yml │ └── template │ └── test.tmpl └── prometheus ├── prometheus.yml └── rules.yml prometheus.yml global: scrape_interval: 15s evaluation_interval: 15s scrape_timeout: 15s external_labels: monitor: 'codelab_monitor' # 报警服务，alertmanager默认运行在9093端口 alerting: alertmanagers: - static_configs: - targets: [\"localhost:9093\"] # 报警规则 rule_files: - \"rules.yml\" # 以下定义了可采集的指标数据源 scrape_configs: # 安装node-exporter并运行后，默认在端口9100对外暴露相关metrics - job_name: 'node' scrape_interval: 5s static_configs: - targets: ['localhost:9100'] # prometheus运行时自身也会提供一些metrics - job_name: 'prometheus' scrape_interval: 5s static_configs: - targets: ['localhost:9090'] # mysql指标由mysqld-exporter导出，默认9104端口 - job_name: 'mysqld' scrape_interval: 5s static_configs: - targets: ['localhost:9104'] rules.yml groups: # 测试规则，当CPU使用率（5分钟）超过20%时，达到报警条件 - name: test-rules rules: - alert: hostCpuUsageAlert expr: sum(avg without (cpu)(irate(node_cpu_seconds_total{mode!='idle'}[5m]))) by (instance) * 100 > 20 for: 1s labels: severity: page annotations: summary: \"Instance {{ $labels.instance }} CPU usgae high\" description: \"{{ $labels.instance }} CPU usage above 20% (current value: {{ $value }})\" alertmanager.yml global: resolve_timeout: 5m #处理超时时间，默认为5min smtp_smarthost: 'smtp.163.com:465' # 邮箱smtp服务器代理 smtp_from: 'xxx@163.com' # 发送邮箱名称 smtp_auth_username: 'xxx@163.com' # 邮箱名称 smtp_auth_password: 'xxxxxxxx' # 邮箱密码或授权码 wechat_api_url: 'https://qyapi.weixin.qq.com/cgi-bin/' # 企业微信地址 # 模板 templates: - 'template/*.tmpl' # 路由 route: group_by: ['alertname'] # 报警分组依据 group_wait: 10s # 第一次等待多久时间发送一组警报的通知 group_interval: 10s # 在发送新警报前的等待时间 repeat_interval: 1m # 发送重复警报的周期 对于email此项不可以设置过低 receiver: 'web.hook' # 使用receivers中name为web.hook的报警方式 # 常用的报警通知方式有3种，webhook、email和企业微信 receivers: # webhook方式 - name: 'web.hook' webhook_configs: # 本地webhook打印报警信息。webhook方式可用于测试，或执行某些动作。 - url: 'http://127.0.0.1:5001/' # 邮件方式 - name: 'email' email_configs: - to: 'xxx@yyy.com' # 收件人 html: '{{ template \"test.html\" . }}' # 邮箱内容html模板 headers: { Subject: \"[WARN] 报警邮件\"} # 邮件主题 # 企业微信方式 - name: 'qywx' webhook_configs: - send_resolved: true to_party: '1' # 接收组的id agent_id: 'xxxxxx' # 企业微信-->自定应用-->AgentId corp_id: 'xxxxxx' # 我的企业-->CorpId[在底部] api_secret: 'xxxxxx' # 企业微信-->自定应用-->Secret message: '{{ template \"test.html\" . }}' # 模板设定 这里不使用docker方式运行exporter，通过下载安装包安装node_exporter和mysqld_exporter。 安装、启动node_exporter wget https://github.com/prometheus/node_exporter/releases/download/v0.17.0/node_exporter-0.17.0.linux-amd64.tar.gz tar -xf node_exporter-0.17.0.linux-amd64.tar.gz cd node_exporter-0.17.0.linux-amd64/ # 运行node_exporter nohup ./node_exporter & # 查看node_exporter监听的端口 [root@zabbix02 node_exporter-0.17.0.linux-amd64]# netstat -alntp |grep node_expo tcp6 0 0 :::9100 :::* LISTEN 21164/./node_export tcp6 0 0 ::1:9100 ::1:57580 ESTABLISHED 21164/./node_export 安装、启动mysqld_exporter wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.11.0/mysqld_exporter-0.11.0.linux-amd64.tar.gz tar -xf mysqld_exporter-0.11.0.linux-amd64.tar.gz cd exporter/mysqld_exporter-0.11.0.linux-amd64/ # 本文中，被监控的mysql与prometheus server在同一个机器上 # 将使用mysql中创建的名为exporter的用户查询mysql指标 cat .my.cnf [client] user=exporter password=exporter EOF # 需创建mysql exporter用户 CREATE USER 'exporter'@'localhost' IDENTIFIED BY 'exporter'; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost'; # flush privileges; # 运行mysqld_exporter nohup ./mysqld_exporter --config.my-cnf=\"./.my.cnf\" & 运行prometheus docker run -d -p 9090:9090 --name prometheus --net=host \\ -v /root/config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\ -v /root/config/prometheus/rules.yml:/etc/prometheus/rules.yml prom/prometheus 运行alertmanager docker run -d -p 9093:9093 --net=host \\ -v /root/config/alertmanager/alertmanager.yml:/etc/alertmanager/config.yml \\ --name alertmanager prom/alertmanager 安装及配置grafana 要通过grafana展示监控数据，见http://pages.platform.development.keep.com/docs/monitoring/prometheus/prom/grafana-intro.html 以上步骤就绪后，通过浏览器访问看看 Prometheus首页 Alerts页面 显示了一条报警规则，说明此前的prometheus rules配置正常。 Targets页面 Alertmanager metrics prometheus的metrics mysql的metrics 功能测试 在Grafana中，导入Prometheus 2.0 Status仪表板，显示如下： 导入Mysql-Overview模板的显示效果： 报警测试： # 先运行一个webhook，前提已安装golang go get github.com/prometheus/alertmanager/examples/webhook webhook # 另开一个窗口，人为拉高CPU使用率，观察webhook打印的日志 cat /dev/zero >/dev/null # 测试完之后，[ctrl]+[c]终止 alertmanager webhook在控制台输出的日志： [root@VM_0_2_centos alertmanager]# webhook 2019/03/15 10:49:32 { > \"receiver\": \"web\\\\.hook\", > \"status\": \"firing\", > \"alerts\": [ > { > \"status\": \"firing\", > \"labels\": { > \"alertname\": \"hostCpuUsageAlert\", > \"instance\": \"cvm001\", > \"monitor\": \"codelab_monitor\", > \"severity\": \"page\" > }, > \"annotations\": { > \"description\": \"cvm001 CPU usage above 20% (current value: 99.80000000001382)\", > \"summary\": \"Instance cvm001 CPU usgae high\" > }, > \"startsAt\": \"2019-03-15T02:49:22.839636267Z\", > \"endsAt\": \"0001-01-01T00:00:00Z\", > \"generatorURL\": \"http://VM_0_2_centos:9090/graph?g0.expr=sum+by%28instance%29+%28avg+without%28cpu%29+%28irate%28node_cpu_seconds_total%7Bmode%21%3D%22idle%22%7D%5B5m%5D%29%29%29+%2A+100+%3E+20\\u0026g0.tab=1\" > } > ], > \"groupLabels\": { > \"alertname\": \"hostCpuUsageAlert\" > }, > \"commonLabels\": { > \"alertname\": \"hostCpuUsageAlert\", > \"instance\": \"cvm001\", > \"monitor\": \"codelab_monitor\", > \"severity\": \"page\" > }, > \"commonAnnotations\": { > \"description\": \"cvm001 CPU usage above 20% (current value: 99.80000000001382)\", > \"summary\": \"Instance cvm001 CPU usgae high\" > }, > \"externalURL\": \"http://VM_0_2_centos:9093\", > \"version\": \"4\", > \"groupKey\": \"{}:{alertname=\\\"hostCpuUsageAlert\\\"}\" >} 2019/03/15 10:52:52 { > \"receiver\": \"web\\\\.hook\", > \"status\": \"resolved\", > \"alerts\": [ > { > \"status\": \"resolved\", > \"labels\": { > \"alertname\": \"hostCpuUsageAlert\", > \"instance\": \"cvm001\", > \"monitor\": \"codelab_monitor\", > \"severity\": \"page\" > }, > \"annotations\": { > \"description\": \"cvm001 CPU usage above 20% (current value: 99.79999999999563)\", > \"summary\": \"Instance cvm001 CPU usgae high\" > }, > \"startsAt\": \"2019-03-15T02:49:22.839636267Z\", > \"endsAt\": \"2019-03-15T02:52:52.839636267Z\", > \"generatorURL\": \"http://VM_0_2_centos:9090/graph?g0.expr=sum+by%28instance%29+%28avg+without%28cpu%29+%28irate%28node_cpu_seconds_total%7Bmode%21%3D%22idle%22%7D%5B5m%5D%29%29%29+%2A+100+%3E+20\\u0026g0.tab=1\" > } > ], > \"groupLabels\": { > \"alertname\": \"hostCpuUsageAlert\" > }, > \"commonLabels\": { > \"alertname\": \"hostCpuUsageAlert\", > \"instance\": \"cvm001\", > \"monitor\": \"codelab_monitor\", > \"severity\": \"page\" > }, > \"commonAnnotations\": { > \"description\": \"cvm001 CPU usage above 20% (current value: 99.79999999999563)\", > \"summary\": \"Instance cvm001 CPU usgae high\" > }, > \"externalURL\": \"http://VM_0_2_centos:9093\", > \"version\": \"4\", > \"groupKey\": \"{}:{alertname=\\\"hostCpuUsageAlert\\\"}\" >} 当CPU使用率高于20%时，Prometheus Alerts页面的报警状态先后变化顺序：in active -> pending -> firing；恢复后，日志显示resolved。 在报警项resolved之前，Alertmanager页面： 通过测试可以看出，webhook方式发出通知是正常的。未测试email和企业微信。 (End) Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/alertmanager.html":{"url":"docs/monitoring/prometheus/prom/alertmanager.html","title":"告警组件——alertmanager","keywords":"","body":"Alertmanager Alertmanager作为一个独立的组件，负责接收并处理来自Prometheus Server(也可以是其它的客户端程序)的告警信息。Alertmanager可以对这些告警信息进行进一步的处理，比如当接收到大量重复告警时能够消除重复的告警信息，同时对告警信息进行分组并且路由到正确的通知方，Prometheus内置了对邮件，Slack等多种通知方式的支持，同时还支持与Webhook的集成，以支持更多定制化的场景。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/grafana-intro.html":{"url":"docs/monitoring/prometheus/prom/grafana-intro.html","title":"图表展示——grafana","keywords":"","body":"Grafana的安装与使用 系统：CentOS 7.5 软件版本：通过docker安装最新版本 简介 官网：https://grafana.com/grafana Grafana是一个跨平台、开源的指标度量分析和可视化工具，提供了强大和优雅的方式去创建、共享、浏览数据。Grafana非常适于和时间序列数据库结合展示数据，如Graphite, InfluxDB, OpenTSDB, Prometheus。 快速安装 # 拉取镜像 docker pull grafana/grafana # 运行grafana（admin登录密码为admin） docker run -d -i -p 3000:3000 \\ -e \"GF_SERVER_ROOT_URL=http://grafana.server.name\" \\ -e \"GF_SECURITY_ADMIN_PASSWORD=admin\" \\ --net=host \\ grafana/grafana 在192.168.100.150上执行，通过浏览器访问：http://192.168.100.150:3000，以admin/admin登录 可以看到只有Install Grafana是已完成状态，待添加数据源、创建仪表盘、添加用户、安装插件等。 官网安装指导：http://docs.grafana.org/installation/docker/> 创建一个Dashboard 先不管Add data source这一步，直接来尝试创建一个Dashboard。 修改数据源 default是grafana提供的默认数据源，作演示用没有实际意义。A系列是随机产生的数据。 修改图表样式 可以在此页签内设置图表类型、线型、阴影、显示的系列等。 修改通用设置 图表标题、描述信息等。 保存bashboard 调整图表宽高，保存自定义的dashboard。 对接Zabbix Grafana没有预装zabbix数据源插件。安装过程如下： 点击Install app & plugins 会跳转到https://grafana.com/plugins，选择zabbix。 由于是通过容器运行grafana的，需进入容器执行插件安装。 # 列出容器 docker ps -a # 进入grafana的容器 docker exec -it bash ## grafana容器内 # 安装grafana的zabbix插件 grafana-cli plugins install alexanderzobnin-zabbix-app exit # 重启容器，然后刷新浏览器页面 docker restart 重启容器后刷新浏览器，可以看到zabbix插件已安装，并启用之。 添加zabbix数据源 要从zabbix平台http://192.168.100.220/zabbix获取数据，URL填http://192.168.100.220/zabbix/api_jsonrpc.php。 保存后，切换到Dashboards页签，导入相关的zabbix插件提供的dashboard。 回到Home，查看已有的dashboard。 查看zabbix监控数据在grafana上的展示效果。 (End) Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/prometheus_federate.html":{"url":"docs/monitoring/prometheus/prom/prometheus_federate.html","title":"联邦","keywords":"","body":"Prometheus federate 示例配置： global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 1m alerting: alertmanagers: - static_configs: - targets: - x.xx.xxx.xxx:9093 scheme: http timeout: 10s rule_files: - /etc/prometheus/rules/*.yml scrape_configs: - job_name: federate honor_labels: true params: match[]: - '{job=~\"\\\\w.*\"}' #- '{job=~\"[a-zA-Z_].*\"}' metrics_path: /federate scheme: http static_configs: - targets: - x.xx.xxx.xxx:9090 - job_name: federate2 honor_labels: true params: match[]: - '{job=~\"\\\\w.*\"}' scrape_interval: 30s scrape_timeout: 30s metrics_path: /federate scheme: http static_configs: - targets: - x.xx.xxx.xxx:xxxx - job_name: federate_pmm_server scrape_interval: 30s scrape_timeout: 30s honor_labels: true metrics_path: /prometheus/federate basic_auth: username: \"xxx\" password: \"xxx\" params: match[]: - '{job=~\"\\\\w.*\"}' static_configs: - targets: - x.xx.xxx.xxx:xxxx Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/prom/prometheus-discovery.html":{"url":"docs/monitoring/prometheus/prom/prometheus-discovery.html","title":"自动发现","keywords":"","body":"Prometheus自动发现配置 Prometheus支持基于Consul自动发现 prometheus.yml: global: scrape_interval: 1s scrape_timeout: 1s evaluation_interval: 5s alerting: alertmanagers: - static_configs: - targets: ['10.1.7.211:9093'] rule_files: - 'rules/*.yml' scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: ['localhost:9090'] labels: instance: prometheus - job_name: consul_sd consul_sd_configs: - server: '192.168.100.140:8500' datacenter: dc01 services: [] relabel_configs: - source_labels: [__meta_consul_tags] regex: '.*,prometheus-target,.*' action: keep - source_labels: [__meta_consul_service] target_label: sn - job_name: consul_sd_dev consul_sd_configs: - server: '10.1.7.211:8500' datacenter: dc1 services: [] relabel_configs: - source_labels: [__meta_consul_tags] regex: '.*prometheus-target.*' action: keep - source_labels: ['__address__'] # regex: '.*:(.*)' regex: '127.0.0.1:(.*)' target_label: '__address__' replacement: '10.1.7.211:$1' Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/":{"url":"docs/monitoring/prometheus/3rd/","title":"常用软件接入Prometheus监控","keywords":"","body":" 第三方应用：MySQL，MongoDB，Redis，RabbitMQ，NginX等 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/mysql-monitor-steps.html":{"url":"docs/monitoring/prometheus/3rd/mysql-monitor-steps.html","title":"MySQL","keywords":"","body":"MySQL监控接入 Percona Monitoring and Management (PMM)是一款开源的MySQL、MongoDB性能监控工具，PMM客户端负责收集DB监控数据，PMM服务端从已连接的客户端拉取数据，并通过第三方软件Grafana展示图表。 安装pmm-client # 安装pmm-client yum install https://www.percona.com/downloads/pmm/1.1.1/binary/redhat/7/x86_64/pmm-client-1.1.1-1.x86_64.rpm 配置服务端 由服务端192.168.100.99管理数据库监控任务。 在mysql端执行： pmm-admin config --server 192.168.100.99 --bind-address= --client-address= --client-name= 示例： pmm-admin config --server 192.168.100.99 --bind-address=192.168.100.100 --client-address=192.168.100.100 --client-name=Production 执行后会保存到文件/usr/local/percona/pmm-client/pmm.yml： [root@b68-docker-prd pmm]# cat /usr/local/percona/pmm-client/pmm.yml server_address: 192.168.100.99 client_address: 192.168.100.100 bind_address: 192.168.100.100 client_name: Production 添加mysql pmm-admin add mysql --user --password --host --port 示例： pmm-admin add mysql --user root --password xxxxxx --host 192.168.100.100 --port 3306 client-production-3306 查看配置 [root@b68-docker-prd pmm]# pmm-admin list pmm-admin 1.1.1 PMM Server | 192.168.100.99 Client Name | Production Client Address | 192.168.100.100 Service Manager | linux-systemd -------------- ----------------------- ----------- -------- ---------------------------------- ------------------------ SERVICE TYPE NAME LOCAL PORT RUNNING DATA SOURCE OPTIONS -------------- ----------------------- ----------- -------- ---------------------------------- ------------------------ mysql:queries client-production-3306 - YES root:***@tcp(192.168.100.100:3306) query_source=perfschema linux:metrics client-production-3306 42000 YES - mysql:metrics client-production-3306 42002 YES root:***@tcp(192.168.100.100:3306) 图表 数据接入成功后 查询分析页面：http://192.168.100.99/qan/ 监控图表页面：http://192.168.100.99/graph 官方文档：https://www.percona.com/doc/percona-monitoring-and-management/conf-mysql.html Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/Prometheus-monitor-mysql-replication.html":{"url":"docs/monitoring/prometheus/3rd/Prometheus-monitor-mysql-replication.html","title":"MySQL主从同步","keywords":"","body":"Prometheus监控MySQL主从同步 利用node_exporter从*.prom文件读取指标数据 获取数据的脚本 getKeys.sh: #!/bin/sh mypath=$(cd `dirname $0`; pwd) cd $mypath echo \"executing all shell scripts ...\" for sh in `ls *.sh |grep -v $0` do echo \"will execute: ./$sh > ${sh}.prom\" eval \"./$sh > ${sh}.prom\" done 获取主从同步延迟数据的脚本pt-heart.sh： out=`pt-heartbeat -D your_dbname --table=heartbeat --check --host=x.x.x.x --port=xx --user=xx --password=xxxxxx --master-server-id=xxx --print-master-server-id` v=`echo \"$out\" |awk '{print $1}'` m=`echo \"$out\" |awk '{print $2}'` echo \"pt_heart{server_id=\\\"$m\\\"} $v\" 具体示例： out=`pt-heartbeat -D test_sync --table=heartbeat --check --host=10.1.7.211 --port=3306 --user=root --password=xxxxxx --master-server-id=100 --print-master-server-id` ... 脚本位置： [root@docker-dev node_exporter_keys]# pwd /srv/node_exporter_keys [root@docker-dev node_exporter_keys]# tree . . ├── getkeys.sh ├── procs.sh ├── procs.sh.prom ├── pt-heart.sh └── pt-heart.sh.prom 设置定时任务 设置crontab定时任务： * * * * * /srv/node_exporter_keys/getkeys.sh 输出的文件pt-heart.sh.prom： pt_heart{server_id=\"100\"} 0.00 修改node_exporter启动参数： 分两种情形： 通过docker运行node_exporter version: '3.0' services: node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro - /srv/node_exporter_keys:/var/extra_keys:ro # 映射到/var/extra_keys ports: - 9100:9100 command: - '--collector.textfile.directory=/var/extra_keys' # 指定从/var/extra_keys读取*.prom - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - '--collector.filesystem.ignored-mount-points=\"^(/rootfs|/host|)/(sys|proc|dev|host|etc)($$|/)\"' - '--collector.filesystem.ignored-fs-types=\"^(sys|proc|auto|cgroup|devpts|ns|au|fuse\\.lxc|mqueue)(fs|)$$\" 通过supervisor运行node_exporter [program:node_exporter] command=/path/to/node_exporter --collector.textfile.directory=/srv/node_exporter_keys directory=/path/to user=root startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/node_exporter.log 最后，访问相应的http://your-host:9100/metrics，查看是否有新增的指标名称pt_heart。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/mongodb-monitor-steps.html":{"url":"docs/monitoring/prometheus/3rd/mongodb-monitor-steps.html","title":"MongoDB","keywords":"","body":"MongoDB监控接入 Percona Monitoring and Management (PMM)是一款开源的MySQL、MongoDB性能监控工具，PMM客户端负责收集DB监控数据，PMM服务端从已连接的客户端拉取数据，并通过第三方软件Grafana展示图表。 安装pmm-client # 安装pmm-client yum install https://www.percona.com/downloads/pmm/1.1.1/binary/redhat/7/x86_64/pmm-client-1.1.1-1.x86_64.rpm 配置服务端 目前统一由PMM服务端10.1.100.200管理数据库监控任务。 在mysql端执行： pmm-admin config --server 10.1.100.200 --bind-address= --client-address= --client-name= 示例： pmm-admin config --server 10.1.100.200 --bind-address=10.1.7.211 --client-address=10.1.7.211 --client-name=Dev 执行后会保存到文件/usr/local/percona/pmm-client/pmm.yml： [root@b68-docker-prd pmm]# cat /usr/local/percona/pmm-client/pmm.yml server_address: 10.1.100.200 client_address: 10.1.7.211 bind_address: 10.1.7.211 client_name: Dev 添加mongodb [root@docker-dev pmm]# pmm-admin add mongodb --help Usage: pmm-admin add mongodb [name] [flags] Examples: pmm-admin add mongodb pmm-admin add mongodb --cluster bare-metal Flags: --cluster string cluster name --uri string MongoDB URI, format: [mongodb://][user:pass@]host[:port][/database][?options] (default \"localhost:27017\") 示例： pmm-admin add mongodb --cluster rs1 --uri mongodb://10.1.7.211:27017 mongodb-dev 查看配置 [root@docker-dev pmm]# pmm-admin list pmm-admin 1.1.1 PMM Server | 10.1.100.200 Client Name | Dev Client Address | 10.1.7.211 Service Manager | linux-systemd ---------------- ---------------- ----------- -------- ------------------------------ ------------------------ SERVICE TYPE NAME LOCAL PORT RUNNING DATA SOURCE OPTIONS ---------------- ---------------- ----------- -------- ------------------------------ ------------------------ ... mongodb:metrics mongodb-dev 42003 YES 10.1.7.211:27017 cluster=rs1 图表 数据接入成功后 监控图表页面：http://10.1.100.200/graph 官方文档：https://www.percona.com/doc/percona-monitoring-and-management/conf-mongodb.html Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/redis-monitor-steps.html":{"url":"docs/monitoring/prometheus/3rd/redis-monitor-steps.html","title":"Redis","keywords":"","body":"Redis监控接入 在Docker中运行的redis，用docker镜像版的redis_exporter更方便。也可以使用二进制版的redis_exporter。 Docker中的Redis 可通过docker镜像运行redis exporter： redis-expo: image: oliver006/redis_exporter ports: - 9121:9121 network_mode: host environment: REDIS_PASSWORD: \"xxxxxxxxxxxxxx\" SERVICE_TAGS: \"prometheus-target,redis-expo\" command: \"--redis.addr redis://127.0.0.1:6379\" VM安装的Redis # 安装redis_exporter wget https://github.com/oliver006/redis_exporter/releases/download/v1.0.0/redis_exporter-v1.0.0.linux-amd64.tar.gz tar -xf redis_exporter-v1.0.0.linux-amd64.tar.gz -C /srv/ 运行redis_exporter: nohup ./redis_exporter --redis.addr \"redis://127.0.0.1:6379\" --redis.password \"xxxxxxxxxx\" --web.listen-address \"0.0.0.0:9121\" & 2>&1 通过supervisor保持redis_exporter运行时，可作如下配置: [program:redis_exporter] command=redis_exporter directory=/srv/redis_exporter-v1.0.0.linux-amd64 user=root environment=REDIS_PASSWORD=\"xxxxxx\",REDIS_ADDR=\"redis://127.0.0.1:6379\",REDIS_EXPORTER_WEB_LISTEN_ADDRESS=\"0.0.0.0:9121\" startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/redis_exporter.log 运行后，验证是否能输出redis监控数据。以192.168.100.200上的redis为例，打开http://192.168.100.200:9121/metrics，并搜索redis_connected_clients，非0正常，表明redis_exporter已连接上redis。 最后，需要将此接口注册到Consul以便Prometheus能自动添加此target。方法见应用接入说明。 图表 redis_exporter使用说明：https://github.com/oliver006/redis_exporter Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/rabbitmq-monitor-steps.html":{"url":"docs/monitoring/prometheus/3rd/rabbitmq-monitor-steps.html","title":"RabbitMQ","keywords":"","body":"RabbitMQ监控接入 在Docker中运行的rabbitmq，用docker镜像版的rabbitmq-exporter更方便。也可以使用二进制版的rabbitmq-exporter。 Docker中的RabbitMQ 可通过docker镜像运行rabbitmq exporter： rabbitmq_exporter: image: kbudde/rabbitmq-exporter ports: - 9429:9429 environment: SERVICE_TAGS: \"prometheus-target,rabbitmq-expo\" RABBIT_URL: \"http://127.0.0.1:15672\" RABBIT_USER: \"guest\" RABBIT_PASSWORD: \"guest\" PUBLISH_PORT: \"9429\" OUTPUT_FORMAT: \"JSON\" # LOG_LEVEL: \"debug\" network_mode: host VM安装的RabbitMQ 获取rabbitmq_exporter： # go编译rabbitmq_exporter go get https://github.com/kbudde/rabbitmq_exporter # 或者wget下载release的二进制文件包 https://github.com/kbudde/rabbitmq_exporter/releases/download/v0.29.0/rabbitmq_exporter-0.29.0.linux-amd64.tar.gz 运行rabbitmq_exporter: RABBIT_URL=\"http://127.0.0.1:15672\" RABBIT_USER=guest RABBIT_PASSWORD=guest ./rabbitmq_exporter 通过supervisor保持rabbitmq_exporter运行时，可作如下配置: [program:rabbitmq-expo] command=/srv/rabbitmq_exporter directory=/srv user=rabbitmq environment=RABBIT_USER=admin,RABBIT_PASSWORD=\"xxxxxx\",OUTPUT_FORMAT=JSON,PUBLISH_PORT=9429,RABBIT_URL=http://127.0.0.1:15672 startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/rabbitmq-expo.log 运行后，检查是否能导出监控数据。以192.168.100.200上的rabbitmq为例，打开http://192.168.100.200:9429/metrics。 最后，需要将此接口注册到Consul以便Prometheus能自动添加此target。方法见应用接入说明。 图表 单节点rabbitmq： rabbitmq集群： rabbitmq_exporter使用说明：https://github.com/kbudde/rabbitmq_exporter Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/3rd/nginx-monitor-steps.html":{"url":"docs/monitoring/prometheus/3rd/nginx-monitor-steps.html","title":"NginX","keywords":"","body":"NginX监控 nginx需安装nginx-module-vts模块，然后通过nginx-vts-exporter输出监控指标。 nginx-module-vts模块 # 获取nginx-module-vts模块源码 cd /opt git clone https://github.com/vozlt/nginx-module-vts # nginx源码包 wget https://nginx.org/download/nginx-1.17.0.tar.gz tar -xf nginx-1.17.0.tar.gz cd nginx-1.17.0 # 编译nginx，加入nginx-module-vts模块 ./configure --add-module=/opt/nginx-module-vts --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-cc-opt='-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' make && make install 配置nginx 修改nginx.conf http { ... vhost_traffic_status_zone; server { listen 8088; location /status { vhost_traffic_status_display; vhost_traffic_status_display_format html; } } ... } server配置示例 upstream jenkins { server 127.0.0.1:8080; } server { listen 80; server_name jenkins.keep.com; #uri访问统计 vhost_traffic_status_filter_by_set_key $uri uri::$server_name; # #国家/地区访问统计，无此模块已注释 # vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name; # #UserAgent 统计 # vhost_traffic_status_filter_by_set_key $filter_user_agent agent::$server_name; vhost_traffic_status_filter_by_set_key $status $server_name; #http code统计 vhost_traffic_status_filter_by_set_key $upstream_addr upstream::backend; #后端转发统计 vhost_traffic_status_filter_by_set_key $remote_port client::ports::$server_name; #请求端口统计 vhost_traffic_status_filter_by_set_key $remote_addr client::addr::$server_name; #请求IP统计 #请求路径统计 location ~ ^/storage/(.+)/.*$ { set $volume $1; vhost_traffic_status_filter_by_set_key $volume storage::$server_name; } location / { proxy_pass http://jenkins; proxy_redirect default; } } 添加到系统服务 /etc/systemd/system/nginx.service: [Unit] Description=The nginx HTTP and reverse proxy server After=network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/run/nginx.pid # Nginx will fail to start if /run/nginx.pid already exists but has the wrong # SELinux context. This might happen when running `nginx -t` from the cmdline. # https://bugzilla.redhat.com/show_bug.cgi?id=1268621 ExecStartPre=/usr/bin/rm -f /run/nginx.pid ExecStartPre=/usr/sbin/nginx -t ExecStart=/usr/sbin/nginx ExecReload=/bin/kill -s HUP $MAINPID KillSignal=SIGQUIT TimeoutStopSec=5 KillMode=process PrivateTmp=true [Install] WantedBy=multi-user.target ln -s /etc/systemd/system/nginx.service /etc/systemd/system/multi-user.target.wants/ 启动、检查、重新加载 systemctl start nginx # 修改过nginx配置后 nginx -t nginx -s reload 查看status 数据转换 通过nginx-vts-exporter将由nginx-module-vts模块提供的nginx监控数据转换成prometheus能直接使用的格式。 转换前：http://192.168.100.150:8088/status/format/json 转换后：http://192.168.100.150:9913/metrics wget https://github.com/hnlq715/nginx-vts-exporter/releases/download/v0.10.3/nginx-vts-exporter-0.10.3.linux-amd64.tar.gz tar -xf nginx-vts-exporter-0.10.3.linux-amd64.tar.gz chown -R nginx:nginx nginx-vts-exporter-0.10.3.linux-amd64 ln -s /srv/nginx-vts-exporter-0.10.3.linux-amd64/nginx-vts-exporter /usr/bin/ # 测试运行exporter nohup nginx-vts-exporter -nginx.scrape_uri=http://localhost:8088/status/format/json 可通过supervisor保持exporter在后台运行： [program:nginx_vts_exporter] command=/usr/bin/nginx-vts-exporter -nginx.scrape_uri=http://localhost:8088/status/format/json user=nginx startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/nginx_vts_exporter.log 注册到consul 将nginx监控数据接口注册到consul，使prometheus能获取该target。 curl命令参考： curl http://192.168.100.140:8500/v1/agent/service/register -X PUT -i -H \"Content-Type:application/json\" -d '{ \"ID\": \"nginx_192.168.100.150\", \"Name\": \"nginx_192_168_100_150\", \"Tags\": [\"nginx\", \"development\", \"prometheus-target\"], \"Address\": \"192.168.100.150\", \"Port\": 9913, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"HTTP\": \"http://192.168.100.150:9913/metrics\", \"Interval\": \"15s\" }, \"IsDeleted\": false, \"Meta\": { \"version\": \"1.0\" }, \"EnableTagOverride\": false }' 为了统一管控，应按【应用接入说明】记录到配置仓库中。 注册成功后多了一条service记录： 图表 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/prometheus/self/prometheus-consul-guide.html":{"url":"docs/monitoring/prometheus/self/prometheus-consul-guide.html","title":"自研应用接入Prometheus监控","keywords":"","body":"自研应用接入Prometheus监控 Prometheus支持从Consul数据源自动发现要监控的服务并自动添加到Prometheus Targets。将各服务待监控指标以服务的形式注册到Consul（集群），便于统一管理维护。 前提条件 项目已集成prometheus exporter库，能提供prometheus支持的指标数据API。 步骤 以下以development环境的ETMS服务为例说明： 确认API正常 ETMS bizhost的metrics：http://192.168.100.145:8001/metrics 注册到Consul Consul服务注册——统一配置仓库：http://git.pro.keep.com/YH/ServiceRegistrationConsul 请基于个人分支提交PR ETMS bizhost相关配置的示例： 为了便于管理，可在ServiceRegistrationConsul/conf.d目录下新建一个目录etms。 新建文件ServiceRegistrationConsul/conf.d/etms/etms_bizhost.json： { \"ID\": \"id_dev_etms_bizhost\", \"Name\": \"dev_etms_bizhost\", \"Tags\": [\"etms\", \"bizhost\", \"development\", \"prometheus-target\"], \"Address\": \"192.168.100.145\", \"Port\": 8001, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"HTTP\": \"http://192.168.100.145:8001/metrics\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"IsDeleted\": false, \"EnableTagOverride\": false } 如果有多个属于etms的服务地址需要注册，比如还有ETMS webhost，那最好一起写到一个文件。 这时，可新建文件ServiceRegistrationConsul/conf.d/etms/etms.json： [{ \"ID\": \"id_dev_etms_bizhost\", \"Name\": \"dev_etms_bizhost\", \"Tags\": [\"etms\", \"bizhost\", \"development\", \"prometheus-target\"], \"Address\": \"192.168.100.145\", \"Port\": 8001, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"HTTP\": \"http://192.168.100.145:8001/metrics\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"IsDeleted\": false, \"EnableTagOverride\": false }, { \"ID\": \"id_dev_etms_webhost\", \"Name\": \"dev_etms_webhost\", \"Tags\": [\"etms\", \"webhost\", \"development\", \"prometheus-target\"], \"Address\": \"192.168.100.145\", \"Port\": 8001, \"Check\": { \"DeregisterCriticalServiceAfter\": \"90m\", \"HTTP\": \"http://192.168.100.145:9001/metrics\", \"Interval\": \"15s\" }, \"Meta\": { \"version\": \"1.0\" }, \"IsDeleted\": false, \"EnableTagOverride\": false }] 验证 查看Consul 在http://192.168.100.140:8500/ui/dc01/services上查看服务是否注册成功。按prometheus的设置，带有prometheus-target标签的服务项会被选择作为一个prometheus target。 查看Prometheus Targets http://prometheus.platform.development.keep.com/targets 指标说明 说明上述metrics接口提供的指标名称和值的含义。 告警条件说明 给出监控数据异常的界定条件，以便添加告警规则。 图表 以下自定义图表展示的是dotnet_collection_count_total的值的变化： dotnet_collection_count_total{generation=\"2\"} N #g2 dotnet_collection_count_total{generation=\"0\"} N #g0 dotnet_collection_count_total{generation=\"1\"} N #g1 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/zabbix/zabbix-intro.html":{"url":"docs/monitoring/zabbix/zabbix-intro.html","title":"Zabbix","keywords":"","body":"Zabbix的安装与使用 Linux环境：CentOS Linux release 7.5.1804 (Core) 安装 安装MySQL 安装mariadb yum -y install mariadb mariadb-server mariadb-devel systemctl start mariadb systemctl enable mariadb mysql_secure_installation # 通过mysql配置向导完成某些设置，也可通过修改表的方式完成设置 配置字符集 在/etc/my.cnf的[mysqld]下添加： init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake 在/etc/my.cnf.d/client.cnf的[client]中添加： default-character-set=utf8 在/etc/my.cnf.d/mysql-clients.cnf的[mysql]中添加： default-character-set=utf8 重启使生效： systemctl restart mariadb 查看字符集： MariaDB [(none)]> show variables like \"%character%\"; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 8 rows in set (0.00 sec) MariaDB [(none)]> show variables like \"%collation%\"; +----------------------+-----------------+ | Variable_name | Value | +----------------------+-----------------+ | collation_connection | utf8_general_ci | | collation_database | utf8_general_ci | | collation_server | utf8_general_ci | +----------------------+-----------------+ 3 rows in set (0.00 sec) MySQL的安装及配置完成。 安装Zabbix Zabbix安装步骤见：CentOS 7安装zabbix 3.0，基于RPM方式安装Zabbix 3.0.25。 如果想通过编译源码安装，可参考Zabbix源码安装过程记录。 另外，安装zabbix_get以便测试： [root@VM_0_6_centos ~]# yum install zabbix-get # 添加监控项时，可在zabbix_server上通过zabbix_get测试命令是否有效 [root@VM_0_6_centos ~]# zabbix_get -s 188.131.133.107 -k vfs.fs.size[/,pused] 17.268719 在安装完zabbix之后，需在MySQL中创建zabbix数据库并导入初始数据表。 安装配置完成并启动所有服务后，以http:///zabbix访问zabbix控制面板，初始用户名/密码为：Admin/zabbix。 监控 以下添加一个被监控主机cvm001，监控其cpu、磁盘、80端口等。 主机 监控项 cvm001 cpu 平均负载、cpu 上下文切换频率 cvm001 根分区剩余空间百分比 cvm001 80端口状态（httpd服务） cvm001 mysql进程数 添加主机 【配置】>【主机】>【创建主机】： 添加监控项 【配置】>【主机】，在刚刚新加的主机所在行，【监控项】>【创建监控项】： 1）CPU负载 设置历史数据保留7天，并为cpu相关的监控定义了一个名为cpu的应用集。 2）CPU上下文切换次数 储存值选择“差量（每秒速率）”： 3）根分区使用率 \"pused\"表示已使用空间所占百分比。 4）mysql的进程 通过proc.num[mysqld,mysql]获取用户mysql的进程mysqld的数目： 5）80端口状态 查看值选择的是Service state，正常状态值是\"Up\"： 如下图，已添加完上面的5个监控项： 查看最新监控数据如下： 查看cpu上下文切换次数的【图形】： 图表标题中文字符显示不出来，是字体的原因，可自行替换，此处从略。 创建触发器 【配置】>【主机】，主机所在行的【触发器】>【创建触发器】： 自定义报警媒介 写一个模拟报警脚本如下，放置在/usr/lib/zabbix/alertscripts/myAlert.sh: #!/bin/sh alert_path=/var/www/html/myalert mkdir -p $alert_path gen_html() { echo \"\"\" Alert \"\"\" echo \"SENDTO:$1\" echo \"SUBJECT:$2\" echo \"MESSAGE:$3\" echo \"\"\" \"\"\" } gen_html $@ > $alert_path/1.html chmod -R 755 $alert_path 并修改/etc/zabbix/zabbix_server.conf配置： AllowRoot=1 User=root 用户-告警绑定 创建动作 模拟报警 模拟报警条件： # 假设当前占用率19%，写一个大小约1GB的文件，使根分区占用率 > 20% 达到触发报警 [root@VM_0_7_centos ~]# dd if=/dev/zero bs=1024 count=1000000 of=/root/1Gb.file 查看监控图形（设定的阈值是16%）： 查看触发器状态： 查看事件记录： 查看报警通知结果： 当报警解除后： 经测试，报警机制正常。正式环境中，配置内置的Email或SMS报警媒介，或使用自定义的邮件、微信、短信告警脚本等替代。 参考 zabbix 3.0 quickstart （End) Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/monitoring/zabbix/zabbix-discovery-rule.html":{"url":"docs/monitoring/zabbix/zabbix-discovery-rule.html","title":"自动发现","keywords":"","body":"通过Zabbix的自动发现（规则）自动创建监控项 下图是一个已配置并启用的自动发现规则: 应用到165.194这个主机后，自动发现的监控项： 服务端配置 创建Template 创建Discovery rule 键值readPorts跟被监控端配置的参数有关，下面再提。 创建监控项原型 宏引用符号{#SERVICE}和{#TCP_PORT}跟被监控端返回的json数据有关。net.tcp.listen[]表示监听目标机器的端口状态，返回值为0或1，0表示Down，1表示Up。另外，$1表示键值里的第一个参数，即。 创建触发器原型 添加完监控项和触发器原型的样子： 最后，添加主机并关联此模板。 被监控端配置 首先，看一下/etc/zabbix目录下的文件。scripts目录是新加的，zabbix_agentd.conf也作了修改。 使用自动发现功能，需要返回符号zabbix要求的json数据： 例如： { \"data\": [ { \"{#SERVICE}\": \"phjd_mysql\", \"{#TCP_PORT}\": \"28001\" }, { \"{#SERVICE}\": \"philips_taxfree_tuangou_payment_management\", \"{#TCP_PORT}\": \"16200\" } ] } 其形式大致如下： { \"data\": [ { \"{#KEY01}\": \"value01_01\", \"{#KEY02}\": \"value01_02\", \"{#KEY03}\": \"value01_03\", ... }, { \"{#KEY01}\": \"value02_01\", \"{#KEY02}\": \"value02_02\", \"{#KEY02}\": \"value02_03\", ... }, ... ] } readPorts.sh脚本的内容很简单，就是读取已准备好的json数据文件ports.json，作为返回给zabbix server的值： [root@172 scripts]# cat readPorts.sh #!/bin/sh mypath=$(cd `dirname $0`; pwd) cat $mypath/ports.json 为了返回给server端，需要再zabbix_agentd.conf中配置： 这样，server端就可以使用readPorts这个键值，以获取写在ports.json中的数据。readPorts下面定义的getPorts、getAllPorts是另外两个可用的自定义键值。 自动发现需监控的服务/端口，难点不在于读取，而在于自动生成json数据文件。此处的ports.json是根据ports.ini转换而来的。 ports.ini文件需人工填写： getPorts.py脚本如下，用于转换ports.ini为ports.json： #!/usr/bin/env python #coding:utf-8 import os, json, re mypath = os.path.dirname(os.path.realpath(__file__)) port_list = [] port_dict = {\"data\":None} with open('%s/ports.ini' % mypath, 'r') as f: for line in f.readlines(): line = line.strip() if line: try: port_srv = line.split(':') port_list.append({\"{#TCP_PORT}\": port_srv[1].strip(), \"{#SERVICE}\": port_srv[0].strip()}) except: pass port_dict[\"data\"] = port_list json_str = json.dumps(port_dict, sort_keys=True, indent=4) # json_str已是json字符串，但每行尾部有一个多余的空格，zabbix不接受，须删除行尾空格 p = re.compile(\"\\s+$\") for line in json_str.split('\\n'): ss = re.sub(p, \"\", line) print ss 转换ports.ini为json形式： 至于getAllPorts.py，它的功能是直接返回主机上实际开放的端口数据： （End) Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/log-analysis/":{"url":"docs/log-analysis/","title":"ELK","keywords":"","body":"日志分析 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/log-analysis/es-xpack.html":{"url":"docs/log-analysis/es-xpack.html","title":"X-pack权限控制版ES","keywords":"","body":"Elasticsearch（启用用户权限控制）镜像构建过程 自动构建&推送ES镜像至Harbor 环境变量： export Harbor_AddRess=x.x.x.x export Harbor_User=xxx export Harbor_Pwd=xxxxxx 构建&推送脚本： #!/bin/sh # 0 mypath=$(cd `dirname $0`; pwd) echo $mypath version=$1 version=${version:=7.5.0} echo \"version: $version\" mkdir $version es_path=$mypath/$version cd $es_path mkdir -p $es_path/{src,build,install} # 1 cd $es_path/src wget https://github.com/elastic/elasticsearch/archive/v${version}.tar.gz tar -xf v${version}.tar.gz cd $es_path/install wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-${version}-linux-x86_64.tar.gz tar -xf elasticsearch-${version}-linux-x86_64.tar.gz # 2 cd $es_path/build # lib module ln -s $es_path/install/elasticsearch-${version}/lib . ln -s $es_path/install/elasticsearch-${version}/modules . # License.java find $es_path/src -name \"License.java\" | xargs -r -I {} cp {} . sed -i 's#this.type = type;#this.type = \"platinum\";#g' License.java sed -i 's#validate();#// validate();#g' License.java # compile License.java javac -cp \"`ls lib/elasticsearch-${version}.jar`:`ls lib/elasticsearch-x-content-${version}.jar`:`ls lib/lucene-core-*.jar`:`ls modules/x-pack-core/x-pack-core-${version}.jar`\" License.java echo \"=========================== License.java: in `pwd`/\" # 3 cd $es_path/build mkdir src && cd src find $es_path/install -name \"x-pack-core-${version}.jar\" | xargs -r -I {} cp {} . jar xvf x-pack-core-${version}.jar rm -f x-pack-core-${version}.jar cp -f ../License*.class org/elasticsearch/license/ jar cvf x-pack-core-${version}.jar . # 4 cat Dockerfile FROM elasticsearch:${version} COPY ./x-pack-core-${version}.jar /usr/share/elasticsearch/modules/x-pack-core/ EOF echo \"docker build elasticsearch by running:\" echo \"docker build -t elasticsearch:${version} .\" echo \"under current directory\" echo echo \"End 升级ELK ES镜像改用：192.168.101.30/yhgit/devops/es-xpack:7.5.1 设置Kibana中文: 修改Kibana配置文件kibana.yml server.name: kibana server.host: \"0\" elasticsearch.hosts: [ \"http://elasticsearch:9200\" ] xpack.monitoring.ui.container.elasticsearch.enabled: true elasticsearch.username: xxx elasticsearch.password: xxxxxx elasticsearch.requestTimeout: 90000 i18n.locale: \"zh-CN\" Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/log-analysis/es-account-manage.html":{"url":"docs/log-analysis/es-account-manage.html","title":"ELK账号权限分组管理","keywords":"","body":"ES分组权限管理的账号设置 基于X-Pack权限控制版ES，给不同项目组、用户分配不同的权限 创建角色 设置角色的ES权限 设置角色的Kibana权限 创建用户 新用户登录 测试ES索引权限 curl -XPUT -u yh:password \"http://es.platform.development.keep.com/yh-es-privilege-test/_doc/1?pretty\" -H 'Content-Type: application/json' -d ' { \"name\": \"John\" }' { \"error\" : { \"root_cause\" : [ { \"type\" : \"security_exception\", \"reason\" : \"action [indices:admin/create] is unauthorized for user [yh]\" } ], \"type\" : \"security_exception\", \"reason\" : \"action [indices:admin/create] is unauthorized for user [yh]\" }, \"status\" : 403 } 权限不够，不能创建该索引。 使用elastic账号查看已创建的role_yh具体有哪些权限： curl -XGET -u elastic:xxxxxx \"http://es.platform.development.keep.com/_xpack/security/role/role_yh\" { \"role_yh\": { \"cluster\": [], \"indices\": [ { \"names\": [ \"yh.*\" ], \"privileges\": [ \"all\" ], \"field_security\": { \"grant\": [ \"*\" ], \"except\": [] }, \"allow_restricted_indices\": false } ], \"applications\": [ { \"application\": \"kibana-.kibana\", \"privileges\": [ \"feature_discover.all\", \"feature_visualize.all\", \"feature_dashboard.all\", \"feature_dev_tools.all\", \"feature_indexPatterns.all\" ], \"resources\": [ \"space:default\" ] } ], \"run_as\": [], \"metadata\": {}, \"transient_metadata\": { \"enabled\": true } } } 由于给role_yh角色分配的ES索引权限是仅名称以yh.开头，故无权创建。改为创建yh.es.test再试则成功： { \"_index\" : \"yh.es.test\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_version\" : 1, \"result\" : \"created\", \"_shards\" : { \"total\" : 2, \"successful\" : 1, \"failed\" : 0 }, \"_seq_no\" : 0, \"_primary_term\" : 1 } 账号规划 根据以上，初步设置了以下账号： 角色 现有账号 ES索引权限 Kibana工作区权限 role_yh yh yh.* 发现 feature_discover.all可视化 feature_visualize.all仪表板 feature_dashboard.all开发工具 feature_dev_tools.all索引模式 eature_indexPatterns.all role_erp erp erp.k8s.erp\\ 同上 role_k8s k8s k8s* 同上 role_etms etms etms* 同上 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/log-analysis/elk-problems.html":{"url":"docs/log-analysis/elk-problems.html","title":"ELK若干问题","keywords":"","body":"ELK问题整理 输出日志多耗尽磁盘空闲空间 /usr/share/elasticsearch/config/log4j2.properties 置logLevel: off es默认分片数1000不够的问题 索引es报如下错误： { \"error\" : { \"root_cause\" : [ { \"type\" : \"validation_exception\", \"reason\" : \"Validation Failed: 1: this action would add [2] total shards, but this cluster currently has [1000]/[1000] maximum shards open;\" } ], \"type\" : \"validation_exception\", \"reason\" : \"Validation Failed: 1: this action would add [2] total shards, but this cluster currently has [1000]/[1000] maximum shards open;\" }, \"status\" : 400 } 解决：设置最大分片数3000 PUT /_cluster/settings { \"transient\": { \"cluster\": { \"max_shards_per_node\":3000 } } } docker-compose限制ELK的CPU、内存 version: '3.7' services: elasticsearch: image: ... ... deploy: resources: limits: cpus: '0.50' memory: 4096M ... 启动命令： docker-compose --compatibility up -d elasticsearch、kibana 通过NginX作Basic Auth认证 upstream kb443 { server 127.0.0.1:5601; } upstream es443 { server 127.0.0.1:9200; } server { listen 443 ssl; ssl_certificate /etc/letsencrypt/live/keep.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/keep.com/privkey.pem; server_name es.keep.com; location / { auth_basic \"secret\"; auth_basic_user_file /etc/nginx/nginx_passwd.kibana; proxy_pass http://es443; proxy_redirect off; proxy_buffering off; proxy_http_version 1.1; proxy_set_header Connection \"Keep-Alive\"; proxy_set_header Proxy-Connection \"Keep-Alive\"; } } server { listen 443 ssl; ssl_certificate /etc/letsencrypt/live/keep.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/keep.com/privkey.pem; server_name kibana.keep.com; root html; index index.html index.htm; access_log /var/log/nginx/kibana-443.access.log main; error_log /var/log/nginx/kibana-443.error.log ; location / { auth_basic \"secret\"; auth_basic_user_file /etc/nginx/nginx_passwd.kibana; proxy_pass http://kb443; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect http://$host http://$host:$server_port; add_header Strict-Transport-Security \"max-age=31536000\"; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } 更新用户名/密码： # yum install httpd-tools -y htpasswd -c -b /etc/nginx/nginx_passwd.kibana $username $password Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/log-analysis/clean_elk_indices.html":{"url":"docs/log-analysis/clean_elk_indices.html","title":"清理ES索引","keywords":"","body":"清理ELK的旧索引 安装elasticsearch-curator pip install elasticsearch-curator 如果报错\"ERROR: Cannot uninstall 'PyYAML'. It is a distutils installed ...\" pip install pip==8.0.3 pip uninstall pyyaml pip install elasticsearch-curator 配置示例： ~/.curator/curator.yml client: hosts: - 10.1.7.211 port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: aws_key: aws_secret_key: aws_region: ssl_no_validate: False http_auth: timeout: 100 master_only: False logging: loglevel: INFO logfile: logformat: default blacklist: [‘elasticsearch’] action文件配置： actions: 1: action: delete_indices description: >- Delete indices older than 10 days (based on index name). options: ignore_empty_list: True timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: pattern kind: prefix value: '[a-z]' #匹配小写字母开头的index exclude: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 10 exclude: 只按时间匹配： actions: 1: action: delete_indices description: >- Delete indices older than 8 days (based on index name). options: ignore_empty_list: True timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 8 exclude: 执行清理： # 命令用法 curator [--config ] [--dry-run] [root@test-vm150 opt]# curator ac.yml 2019-08-19 12:02:22,174 INFO Preparing Action ID: 1, \"delete_indices\" 2019-08-19 12:02:22,191 INFO GET http://10.1.7.211:9200/ [status:200 request:0.011s] 2019-08-19 12:02:22,191 INFO Trying Action ID: 1, \"delete_indices\": Delete indices older than 30 days (based on index name). ... 2019-08-19 12:02:22,762 INFO Deleting 7 selected indices: [u'yhcorests-log-2019.08.08', u'yhcorests-log-2019.08.09', u'yhweb.mdmadapter-log-2019.08.09', u'fms-log-2019.08.08', u'fms-log-2019.08.09', u'yh_onecardsolution-log-2019.08.08', u'yh_onecardsolution-log-2019.08.09'] 2019-08-19 12:02:22,762 INFO ---deleting index yhcorests-log-2019.08.08 2019-08-19 12:02:22,763 INFO ---deleting index yhcorests-log-2019.08.09 2019-08-19 12:02:22,763 INFO ---deleting index yhweb.mdmadapter-log-2019.08.09 2019-08-19 12:02:22,763 INFO ---deleting index fms-log-2019.08.08 2019-08-19 12:02:22,763 INFO ---deleting index fms-log-2019.08.09 2019-08-19 12:02:22,763 INFO ---deleting index yh_onecardsolution-log-2019.08.08 2019-08-19 12:02:22,763 INFO ---deleting index yh_onecardsolution-log-2019.08.09 2019-08-19 12:02:23,049 INFO DELETE http://10.1.7.211:9200/fms-log-2019.08.08,fms-log-2019.08.09,yh_onecardsolution-log-2019.08.08,yh_onecardsolution-log-2019.08.09,yhcorests-log-2019.08.08,yhcorests-log-2019.08.09,yhweb.mdmadapter-log-2019.08.09?master_timeout=100s [status:200 request:0.286s] ... Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/cicd/":{"url":"docs/cicd/","title":"CI/CD","keywords":"","body":"持续集成 GitLab CI/CD ... Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/cicd/gitlab/":{"url":"docs/cicd/gitlab/","title":"GitLab CI/CD","keywords":"","body":" GitLab CI/CD是GitLab内置的工具，用于项目持续集成、持续部署。GitLab CI/CD 由位于代码仓库根目录的名为.gitlab-ci.yml的文件配置。而此文件中设置的脚本由GitLab Runner执行。 本书 通过GitLab CI/CD自动发布： Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/cicd/gitlab/gitlab-runner.html":{"url":"docs/cicd/gitlab/gitlab-runner.html","title":"GitLab Runner","keywords":"","body":"添加Gitlab Runner 启动gitlab-runner（提供自动创建、销毁runner的服务） docker run -d --name gitlab-runner --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest 注册runner gitlab-runner register -n \\ --url http://git.pro.keep.com/ \\ --registration-token your-token*** \\ --executor docker \\ --description \"Docker in Docker Runner\" \\ --docker-image alpine:latest \\ --tag-list \"dind-v19\" 查看注册后生成的/etc/gitlab-runner/config.toml： [[runners]] name = \"Docker in Docker Runner\" url = \"http://git.pro.keep.com/\" token = \"xxxxx\" executor = \"docker\" [runners.custom_build_dir] [runners.docker] tls_verify = false image = \"alpine:latest\" privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\"/cache\"] shm_size = 0 [runners.cache] [runners.cache.s3] [runners.cache.gcs] [runners.custom] run_exec = \"\" gitlab cicd日志： Running with gitlab-runner 12.1.0 (de7731dd) on Docker in Docker Runner PdsMyiEk Using Docker executor with image docker:stable ... Pulling docker image docker:stable ... Using docker image sha256:9a38a85b1e4e7bb53b7c7cc45afff6ba7b1cdfe01b9738f36a3b4ad0cdb10b00 for docker:stable ... Running on runner-PdsMyiEk-project-238-concurrent-0 via 521aa7f3c946... Fetching changes... Initialized empty Git repository in /builds/devops/docs/.git/ Created fresh repository. From http://git.pro.keep.com/devops/docs - [new branch] master -> origin/master Checking out 78455dad as master... Skipping Git submodules setup $ docker info Client: Debug Mode: false Server: ERROR: Cannot connect to the Docker daemon at tcp://docker:2375. Is the docker daemon running? errors pretty printing info ERROR: Job failed: exit code 1 要支持docker in docker，改为： [[runners]] name = \"Docker in Docker Runner\" url = \"http://git.pro.keep.com/\" token = \"PdsMyiEkyfbPsVZgboPd\" executor = \"docker\" [runners.custom_build_dir] [runners.docker] tls_verify = false image = \"alpine:latest\" privileged = true disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\"/cache\", \"/var/run/docker.sock:/var/run/docker.sock\", \"/etc/default/docker:/etc/default/docker\", \"/etc/docker/daemon.json:/etc/docker/daemon.json\"] shm_size = 0 [runners.cache] [runners.cache.s3] [runners.cache.gcs] [runners.custom] run_exec = \"\" Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/cicd/gitlab/gitlab-cicd.html":{"url":"docs/cicd/gitlab/gitlab-cicd.html","title":"GitLab CI/CD","keywords":"","body":"Gitlab CI -- gitlab ci 配置文件 Gitlab CI （持续集成）自动构建任务需编写配置文件 .gitlab-ci.yml 以本书的.gitlab-ci.yml为例： stages: - build - deploy build: image: docker:stable stage: build tags: - dind-v19 script: - docker info - image_name=lzzeng/docs:latest - docker build -t $image_name . - docker login $Harbor_AddRess -u $Harbor_User -p $Harbor_Pwd - docker push $image_name only: - master # this job will affect only the 'master' branch ansible-docker: image: ansible-alpine:2.8.0 stage: deploy tags: - dind-v19 script: - echo \"GITBOOK_HOST is $GITBOOK_HOST_DEV\" - echo \"$SSH_PRIVATE_KEY_DEV\" > ~/.key - chmod 600 ~/.key - echo \"$ANSIBLE_CFG_CONTENT\" > ~/.ansible.cfg - ansible-playbook -i \"$GITBOOK_HOST_DEV,\" --private-key ~/.key -u root deploy/inDocker.yml 解释： stages 段落表示有 有两个构建阶段，标识分别是build 和 deploy build段落，stage域（或属性）的值为 build, 表示这段脚本是上述build的具体执行过程 tags域为 dind-v19， 表示本阶段构建选择tag为dind-v19的gitlab runner，这是事先已注册的一个允许构建docker镜像的执行器，且这个执行器自身也是docker容器，即 docker in docker (dind） build的script段：docker build 构建镜像 -> docker login 登录Harbor -> docker push 推送镜像至Harbor only master 表示只接受master分支更新后触发CI ansible-docker阶段的作用是远程登录目标机器拉取镜像、创建并运行容器 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/config-management/":{"url":"docs/config-management/","title":"CMDB","keywords":"","body":"配置管理 配置管理工具可以提高应用部署和变更的效率，还可以让这些流程变得可重用、可扩展、可预测，甚至让它们维持在期望的状态，从而让资产的可控性提高。 使用配置管理工具的优势还包括： 让代码遵守编码规范，提高代码可读性； 具有 幂等性(Idempotency)，也就是说，无论执行多少次重复的配置管理操作，得到的结果都是一致的； 分布式的设计可以方便地管理大量的远程服务器。 配置管理工具主要分为 拉取(pull)模式和 推送(push)模式。拉取模式是指安装在各台服务器上的 代理(agent)定期从 中央存储库(central repository)拉取最新的配置并应用到对应的服务器上；而推送模式则由 中央服务器(central server)的中央服务器会触发其它受管服务器的更新。 下列是几款开源配置管理工具，全都具有开源许可证、使用外部配置文件、支持无人值守运行、可以通过脚本自定义运行。 Ansible “Ansible 是一个极其简洁的 IT 自动化平台，可以让你的应用和系统以更简单的方式部署。不需要安装任何代理，只需要使用 SSH 的方式和简单的语言，就可以免去脚本或代码部署应用的过程。”——GitHub Ansible 代码库 由于 Ansible 不需要代理，因此对服务器的资源消耗会很少。Ansible 默认使用的推送模式需要借助 SSH 连接，但 Ansible 也支持拉取模式。剧本 可以使用最少的命令集编写，当然也可以扩展为更加精细的自动化任务，包括引入角色、变量和其它人写的模块。 Puppet “Puppet 是一个可以在 Linux、Unix 和 Windows 系统上运行的自动化管理引擎，它可以根据集中的规范来执行诸如添加用户、安装软件包、更新服务器配置等等管理任务。”——GitHub Puppet 代码库 官网 文档 社区 Puppet 作为一款面向运维工程师和系统管理员的工具，在更多情况下是作为配置管理工具来使用。它通过客户端-服务端的模式工作，使用代理从主服务器获取配置指令。 Puppet 使用 声明式语言(declarative language)或 Ruby 来描述系统配置。它包含了不同的模块，并使用 清单文件(manifest files)记录期望达到的目标状态。Puppet 默认使用推送模式，但也支持拉取模式。 Salt “为大规模基础结构或应用程序实现自动化管理的软件。”——GitHub Salt 代码库 官网 文档 社区 Salt 的专长就是快速收集数据，即使是上万台服务器也能够轻松完成任务。它使用 Python 模块来管理配置信息和执行特定的操作，这些模块可以让 Salt 实现所有远程操作和状态管理。但配置 Salt 模块对技术水平有一定的要求。 Salt 使用客户端-服务端的结构（Salt minions 是客户端，而 Salt master 是服务端），并以 Salt 状态文件记录需要达到的目标状态。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/config-management/ansible-tower.html":{"url":"docs/config-management/ansible-tower.html","title":"Ansible Tower","keywords":"","body":"Ansible Tower Ansible Tower (以前叫’AWX’)是能够帮助任何IT团队更容易使用Ansible的解决方案。该方案基于web。 Tower允许对用户进行权限控制，即使某用户不能传送某SSH凭证，你也可以通过Tower来对该用户共享该凭证。我们可以通过图形化界面来管理Inventory，也可以对各种各样的云资源做同步。Tower可以记录所有job的日志，也可以与LDAP集成，并且拥有强大的可浏览的REST API。Tower也提供了命令行工具，可以与Jenkins轻松集成。Provisioning回调对自动伸缩拓扑图提供了强大的支持。 Tower的免费版本最多支持10个节点。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/config-management/bk-cmdb.html":{"url":"docs/config-management/bk-cmdb.html","title":"BlueKing","keywords":"","body":" 蓝鲸配置平台（蓝鲸CMDB）是一个面向资产及应用的企业级配置管理平台。 蓝鲸配置平台提供了全新自定义模型管理，用户不仅可以方便地实现内置模型属性的拓展，同时也能够根据不同的企业需求随时新增模型和关联关系，把网络、中间件、虚拟资源等纳入到CMDB的管理中。除此之外还增加了更多符合场景需要的新功能：机器数据快照、数据自动发现、变更事件主动推送、更加精细的权限管理、可拓展的业务拓扑等功能。 在技术构建上，架构的核心聚焦于资源，我们把CMDB管理的原子资源分为主机、进程和通用对象三种类型，并构建了对这些资源的原子操作层。在这些原子操作之上，我们构建了更贴近用户操作的场景层，场景层通过对不同资源的组合操作来完成用户的请求。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/task-scheduling/":{"url":"docs/task-scheduling/","title":"简介","keywords":"","body":"任务调度 分布式调度在互联网企业中占据着十分重要的作用，尤其是电子商务领域，由于存在数据量大、高并发的特点，对数据处理的要求较高，既要保证高效性，也要保证准确性和安全性，相对比较耗时的业务逻辑往往会从中剥离开来进行异步处理。 . 下列是几款优秀和极具潜力的国产开源分布式任务调度系统： opencron opencron 是一个功能完善且通用的开源定时任务调度系统，拥有先进可靠的自动化任务管理调度功能，提供可操作的 web 图形化管理满足多种场景下各种复杂的定时任务调度，同时集成了 linux 实时监控、webssh 等功能特性。 LTS LTS，light-task-scheduler，是一款分布式任务调度框架, 支持实时任务、定时任务和 Cron 任务。有较好的伸缩性和扩展性，提供对 Spring 的支持（包括 Xml 和注解），提供业务日志记录器。支持节点监控、任务执行监、JVM 监控，支持动态提交、更改、停止任务。 XXL-JOB XXL-JOB 是一个轻量级分布式任务调度框架，支持通过 Web 页面对任务进行 CRUD 操作，支持动态修改任务状态、暂停/恢复任务，以及终止运行中任务，支持在线配置调度任务入参和在线查看调度结果。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/task-scheduling/xxl-job.html":{"url":"docs/task-scheduling/xxl-job.html","title":"XXL-JOB","keywords":"","body":"xxl-job 新增执行器 编译执行器项目 源码：https://github.com/xuxueli/xxl-job/ xxl-job-executor-sample是通过源码配置、编译后的组件xxl-job-executor-sample-springboot运行时提供的一个server，源码中修改的位置如下： 执行mvn clean package编译打包可生成jar文件 然后，通过supervisor使其在后台持续运行。 添加执行器到xxl-job-admin 注意：AppName与编译源码时配置的appname一致，才可以自动获取机器地址（端口）。否则，手动录入。 添加示例任务 任务设置 0 * * * * ? *表示每分钟（0秒时）调度一次，因间隔短，不设置任务超时及失败重试。任务参数内填多行也是作为一个参数（$1）传递给脚本的。路由策略“第一个”表示总是在第一个OnLine机器上执行。 该任务的示例脚本如下： #!/bin/bash echo \"xxl-job: hello shell\" echo \"脚本位置：$0\" echo \"任务参数：$1\" echo \"分片序号 = $2\" echo \"分片总数 = $3\" failed=0 succeed=0 failed_urls=\"\" for url in $1 # 对每一行的url do echo \"current url: [$url]\" status_code=$(curl -s -o /dev/null -w \"%{http_code}\" $url) # 检测http响应状态码 echo \"status_code: $status_code\" if [ \"$status_code\" -ne 200 ]; then echo \"Failed: Access $url failed.\" failed_urls=\"$failed_urls,$url\" let failed=failed+1 # 失败+1 else let succeed=succeed+1 # 成功+1 fi done echo \"Good bye!\" echo \"Total: $failed failed $succeed succeed\" # 日志中打印统计结果 if [ $failed -lt 0 ]; then echo \"Failed Urls: $failed_urls\" fi exit $failed # 退出状态值，非0表示失败 如果执行状态失败，将发出报警邮件。 Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/task-scheduling/upgrade-xxl-job.html":{"url":"docs/task-scheduling/upgrade-xxl-job.html","title":"XXL-JOB升级","keywords":"","body":"编译xxl-job-2.1.2 xxl-job-2.0.2 只有一个admin用户，不能设置创建多个账号。v2.1.2可以设置多个账号。 运行一个maven容器 docker-compose.yml如下： mvn: image: maven:3.5-jdk-8-alpine volumes: - ./data:/data # 源码包xxl-job-2.1.2.tar.gz在./data下 command: [\"tail\", \"-f\", \"/dev/null\"] 或者 docker run -d --name mvn3 maven:3.5-jdk-8-alpine tail -f /dev/null # 复制源码包到容器/data目录中 docker cp data/xxl-job-2.1.2.tar.gz mvn3:/data/ 进入容器编译 docker exec -it mvn3 bash # 编包 cd /data/xxl-job-{版本号} mvn clean package # 拷包 mkdir target mv xxl-job-admin/target/*.jar target/ mv xxl-job-executor-samples/xxl-job-executor-sample-springboot/target/*.jar target/ 将其从容器中拷贝到宿主机目录下： 停止xxl-job，替换原相应的所有jar包并重新启动。 升级失败处理 替换jar包并重新启动，发现启动失败。经分析是版本之间 数据库 的设计发生较大变化。因未提供数据迁移功能，通过手动复制旧版本的所有任务到新版本重建任务，完成本次升级过程。 Supervisord配置 执行器（作为客户端）： [program:xxl-job-executor] command=java -jar /srv/xxl-job-executor-sample-springboot-2.1.2.jar --server.port=10080 --xxl.job.executor.ip=10.1.7.211 --xxl.job.executor.port=9999 --xxl.job.admin.addresses=https://job.keep.com/xxl-job-admin --xxl.job.executor.appname=xxl-job-executor-7-211 directory=/srv user=root startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/xxl-job-executor.log 调度服务（作为服务端）： [program:xxl-job-admin_212] command=java -jar /srv/xxl-target-2.1.2/xxl-job-admin-2.1.2.jar --spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?Unicode=true&characterEncoding=UTF-8 --spring.datasource.username=root --spring.datasource.password=mysql@dmin --server.port=8080 directory=/srv user=root startsecs=3 redirect_stderr=true stdout_logfile_maxbytes=50MB stdout_logfile_backups=3 stdout_logfile=/var/log/xxl-job-admin.log Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/my-gitbook.html":{"url":"docs/my-gitbook.html","title":"制作","keywords":"","body":"使用GitBook 文档结构（示例） [root@VM_0_13_centos docs]# tree -L 1 . . |-- book.json |-- docs |-- node_modules # 可不预先安装js依赖 |-- README.md `-- SUMMARY.md 基于node:alpine构建gitbook-cli镜像 Dockerfile: FROM node:alpine RUN npm install gitbook-cli -g && \\ gitbook fetch 3.2.3 WORKDIR /opt/data 部署gitbook compose: version: '3' services: pages: image: mynode:alpine build: . environment: SERVICE_NAME: pages SERVICE_TAGS: \"prometheus-target,pages,http\" command: [\"sh\", \"-c\", \"cd /opt/data && gitbook install && gitbook serve\"] volumes: - ./docs:/opt/data ports: - 4000:4000 如果未事先下载好相关依赖，启动后gitbook install过程需要一定的时间。 node:alpine的entrypoint是默认的docker-entrypoint.sh，构建gitbook-cli镜像时未另外指定。 docker-entrypoint.sh: #!/bin/sh set -e if [ \"${1#-}\" != \"${1}\" ] || [ -z \"$(command -v \"${1}\")\" ]; then set -- node \"$@\" fi exec \"$@\" 因此，compose中command的以下写法均可： command: - /bin/sh - -c - | cd /opt/data #当镜像默认workdir不是/opt/data时，可在command中cd；是则不必cd gitbook serve command: - sh - -c - | gitbook serve command: [\"gitbook\", \"serve\"] command: [\"sh\", \"-c\", \"gitbook serve\"] command: [\"sh\", \"-c\", \"cd /opt/data && gitbook serve\"] command: [sh, -c, \"gitbook serve\"] command: [sh, -c, gitbook serve] command: [sh, -c, cd /opt/data && gitbook serve] command: sh -c \"cd /opt/data && gitbook serve\" Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/cicd/publish-gitbook.html":{"url":"docs/cicd/publish-gitbook.html","title":"发布","keywords":"","body":"通过Gitlab发布Gitbook 安装git客户端 Windows版Git客户端：https://gitforwindows.org/ 创建gitbook项目docs gitbook项目docs的目录结构： docs # 仓库根目录 ├── book.json # gitbook配置文件 ├── deploy/ # 存放部署脚本 ├── .gitlab-ci.yml # gitlab ci配置文件 ├── Dockerfile # docker镜像编译脚本 ├── docs/ # 文档集中存放目录 │ ├── assets/ # 图片目录 │ ├── cicd/ # 持续集成 │ ├── deployment/ # 应用部署 │ ├── task-sheduling/ # 任务调度 │ ├── config-management/ # 配置管理 │ └── monitoring/ # 运维监控 ├── README.md # 书籍首页 ├── styles/ # css文件 └── SUMMARY.md # 书籍目录文件 克隆gitbook项目 已安装好git客户端，空白处右键如图打开git客户端： git clone 项目文件到本地： 添加一个文档 以下在原项目的docs/else目录下添加一个文档es-maximum-shards.md 并修改书籍目录 添加修改、提交、推送 查看、添加改动 提交到本地版本库、推送到远程版本库（master分支） git commit 提交时，\"-m\"参数写一点简明的备注信息。 自动构建、发布 git push成功之后，在gitlab上可以查看构建进度： Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "},"docs/HELP.html":{"url":"docs/HELP.html","title":"官方示例","keywords":"","body":"GitBook Example GitBook website using GitLab Pages. Learn more about GitLab Pages at https://pages.gitlab.io and the official documentation https://docs.gitlab.com/ce/user/project/pages/. Table of Contents generated with DocToc GitLab CI Building locally GitLab User or Group Pages Did you fork this project? Troubleshooting GitLab CI This project's static Pages are built by GitLab CI, following the steps defined in .gitlab-ci.yml: # requiring the environment of NodeJS 8.9.x LTS (carbon) image: node:8.9 # add 'node_modules' to cache for speeding up builds cache: paths: - node_modules/ # Node modules and dependencies before_script: - npm install gitbook-cli -g # install gitbook - gitbook fetch latest # fetch latest stable version - gitbook install # add any requested plugins in book.json #- gitbook fetch pre # fetch latest pre-release version #- gitbook fetch 2.6.7 # fetch specific version # the 'pages' job will deploy and build your site to the 'public' path pages: stage: deploy script: - gitbook build . public # build to public path artifacts: paths: - public only: - master # this job will affect only the 'master' branch Building locally To work locally with this project, you'll have to follow the steps below: Fork, clone or download this project Install GitBook npm install gitbook-cli -g Fetch GitBook's latest stable version gitbook fetch latest Preview your project: gitbook serve Add content Generate the website: gitbook build (optional) Push your changes to the master branch: git push Read more at GitBook's documentation. GitLab User or Group Pages To use this project as your user/group website, you will need one additional step: just rename your project to namespace.gitlab.io, where namespace is your username or groupname. This can be done by navigating to your project's Settings. Read more about user/group Pages and project Pages. Did you fork this project? If you forked this project for your own use, please go to your project's Settings and remove the forking relationship, which won't be necessary unless you want to contribute back to the upstream project. Troubleshooting CSS is missing! That means two things: Either that you have wrongly set up the CSS URL in your templates, or your static generator has a configuration option that needs to be explicitly set in order to serve static assets under a relative URL. Forked from @virtuacreative Copyright © Zeng 2019 all right reserved，powered by GitbookModified @ 2020-02-05 18:07:05 "}}